[{"id":0,"href":"/coding/docs/distributed/define/","title":"01. 分布式简介","section":"分布式","content":" 极客时间.分布式技术原理与算法解析 笔记\n分布式定义 # 分布式其实就是将相同或相关的程序运行在多台计算机上，从而实现特定目标的一种计算方式.\n分布式形态\n数据并行 任务并行 分布式驱动力量\n性能 可用性 可扩展性 指标 # 性能 # 吞吐量 QPS（Queries Per Second） TPS（Transactions Per Second） BPS（Bits Per Second） 响应时间 完成时间 资源占用 # 空载资源占用 满载资源占用 可用性 # 系统的可用性可以用系统停止服务的时间与总的时间之比衡量\n某功能的失败次数与总的请求次数之比来衡量\n可扩展性 # 当任务的需求随着具体业务不断提高时，除了升级系统的性能做垂直 / 纵向扩展外， 另一个做法就是通过增加机器的方式去水平 / 横向扩展系统规模。\n系统可扩展性的常见指标是加速比（Speedup），也就是一个系统进行扩展后相对扩展前的性能提升\n不同场景下分布式系统的指标\n电商系统 //重吞吐量 IoT //资源占用指标,可以资源占用KB级的 电信业务 // 响应时间、完成时间，以及可用性 HPC // 任务执行时间极长,水平扩展来提高系统的加速比 大数据 // 扩展性 云计算 // 减少用户操作时间,降低系统资源开销 区块链 // 吞吐量和完成时间 "},{"id":1,"href":"/coding/docs/distributed/coordination/","title":"02. 协调与同步","section":"分布式","content":" 极客时间.分布式技术原理与算法解析 笔记\n分布式互斥 # 在分布式系统里，排他性的资源访问方式，叫作分布式互斥（Distributed Mutual Exclusion）， 而这种被互斥访问的共享资源就叫作临界资源（Critical Resource）\n集中式算法 # 协调者参与\ngraph TB subgraph 分布式系统 A[程序 A] --\u003e C[协调者] B[程序 B] --\u003e C[协调者] end subgraph 互斥算法 C[协调者] --\u003e D[发送请求] D --\u003e E[检查资源状态] E --\u003e |资源空闲| F[授权访问] E --\u003e |资源占用| G[排号等待] G --\u003e H[接收通知] H --\u003e D F --\u003e I[访问资源] I --\u003e J[释放资源] J --\u003e |通知协调者| C end 优点\n开发和实现简单,不存在节点间的协调问题 数据一致性易于控制 故障修复相对简单 缺点\n可靠性和性能依赖于中心节点 中心节点出故障或压力过大会导致整体服务挂掉 难以水平扩展以提升吞吐量 分布式算法 # 定义\n当一个程序要访问临界资源时，先向系统中的其他程序发送一条请求消息，在接收到所有程序返回的同意消息后，才可以访问临界资源。其中，请求消息需要包含所请求的资源、请求者的 ID，以及发起请求的时间\nsequenceDiagram actor 程序1 actor 程序2 actor 程序3 程序1-\u003e\u003e程序2: 请求访问资源A 程序1-\u003e\u003e程序3: 请求访问资源A 程序2--\u003e\u003e程序1: 同意 程序3--\u003e\u003e程序1: 同意 程序3-\u003e\u003e程序1: 请求访问资源A 程序3-\u003e\u003e程序2: 请求访问资源A 程序2--\u003e\u003e程序3: 同意 Note over 程序3: 等待程序1使用完 程序1--\u003e\u003e程序3: 同意 程序1-\u003e\u003e资源A: 使用资源A 程序1--\u003e\u003e资源A: 释放资源A 程序3-\u003e\u003e资源A: 使用资源A 程序3--\u003e\u003e资源A: 释放资源A 在大型系统中使用分布式算法，消息数量会随着需要访问临界资源的程序数量呈指数级增加，容易导致高昂的\u0026quot;沟通成本\u0026quot;。\n分布式算法适合节点数目少且变动不频繁的系统，且由于每个程序均需通信交互，因此适合 P2P 结构的系统.\nHadoop # HDFS 的文件修改 sequenceDiagram participant 计算机 1 participant 计算机 2 participant 计算机 3 计算机 1 -\u003e\u003e 计算机 2: 发送文件修改请求 计算机 1 -\u003e\u003e 计算机 3: 发送文件修改请求 计算机 2 --\u003e\u003e 计算机 1: 同意请求 计算机 3 --\u003e\u003e 计算机 1: 同意请求 计算机 1 -\u003e\u003e D文件: 开始修改文件 D文件 --\u003e\u003e 计算机 1: 修改完成 计算机 1 -\u003e\u003e 计算机 2: 发送修改完成消息和文件数据 计算机 1 -\u003e\u003e 计算机 3: 发送修改完成消息和文件数据 计算机 2 -\u003e\u003e E文件: 接收文件数据 计算机 3 -\u003e\u003e F文件: 接收文件数据 分布式算法是一个“先到先得\u0026quot;和“投票全票通过\u0026quot;的公平访问机制，但通信成本较高，可用性也比集中式算法低，适用于临界资源使用频度较低，且系统规模较小的场景.\n令牌环算法 # graph TD A -- 拥有令牌时访问 --\u003e 资源 B -- 拥有令牌时访问 --\u003e 资源 C -- 拥有令牌时访问 --\u003e 资源 D -- 拥有令牌时访问 --\u003e 资源 A -. 传递令牌 .-\u003e B B -. 传递令牌 .-\u003e C C -. 传递令牌 .-\u003e D D -. 传递令牌 .-\u003e A 令牌环算法的公平性高，在改进单点故障后，稳定性也很高，适用于系统规模较小，并且系统中每个程序使用临界资源的频率高且使用时间比较短的场景.\n分布式互斥作用 # 保证资源的独占性访问 并发控制 传统单机上的互斥方法，为什么不能用于分布式环境呢？\n传统单机上的互斥方法,例如mutex锁,不能直接用于分布式环境主要有以下几个原因:\n分布式环境下,进程/线程在不同主机上运行,无法直接访问共享内存区域进行锁操作。 互斥锁依赖于共享内存模型无法实现。\n不同主机上的时钟不一定同步,无法判断锁的持有时间。 互斥锁依赖于同步时钟实现锁定策略。\n不同主机之间的通信开销很高,每次获取锁或释放锁都需要网络传输,性能很差。\n分布式系统里硬件错误或网络故障很常见,单一锁持有者可能crash掉从而阻塞其他进程。缺乏容错能力。\n分布式系统中加入和退出节点很动态,锁机制需要能够自动适应节点变化。\n不同数据中心的节点需要协调进行锁操作,但网络延迟非常高。\n分布式环境需要使用额外的技术如共识算法、消息传递等来实现分布式锁, 可以在不同节点间进行通信和协调,获得更强的一致性和可扩展性。\n分布式选举 # 选举的作用\n选出一个主节点，由它来协调和管理其他节点，以保证集群有序运行和节点间数据的一致性.\n选举算法\n基于序号选举的算法 Bully 算法 多数派算法 Raft 算法 ZAB 算法 Bully 算法 # 选举原则\n在所有活着的节点中，选取 ID 最大的节点作为主节点. 前提\n集群中每个节点均知道其他节点的 ID.\ngraph LR A[节点 A] B[节点 B] C[节点 C] D[节点 D-故障] B --\u003e|1. Election| D B --\u003e|1. Election| C C --\u003e|2. Alive| B C --\u003e|3. Election| D C --\u003e|4. Victory| B C --\u003e|4. Victory| A 消息类型\nElection 消息，用于发起选举 Alive 消息，对 Election 消息的应答 Victory 消息，竞选成功的主节点向其他节点发送的宣誓主权的消息 流程\n集群中每个节点判断自己的 ID 是否为当前活着的节点中 ID 最大的，如果是，则直接向其他节点发送 Victory 消息，宣誓自己的主权； 如果自己不是当前活着的节点中 ID 最大的，则向比自己 ID 大的所有节点发送 Election 消息，并等待其他节点的回复； 若在给定的时间范围内，本节点没有收到其他节点回复的 Alive 消息，则认为自己成为主节点，并向其他节点发送 Victory 消息，宣誓自己成为主节点；若接收到来自比自己 ID 大的节点的 Alive 消息，则等待其他节点发送 Victory 消息； 若本节点收到比自己 ID 小的节点发送的 Election 消息，则回复一个 Alive 消息，告知其他节点，我比你大，重新选举。 优点\n选举速度快 算法复杂度低 简单易实现 缺点\n每个节点有全局的节点信息 任意一个比当前主节点 ID 大的新节点或节点故障后恢复加入集群的时候，都可能会触发重新选举，成为新的主节点，如果该节点频繁退出、加入集群，就会导致频繁切主 Raft 算法 # ZAB 算法 # ZAB（ZooKeeper Atomic Broadcast）选举算法\n是为 ZooKeeper 实现分布式协调功能而设计的。相较于 Raft 算法的投票机制，ZAB 算法增加了通过节点 ID 和数据 ID 作为参考进行选主，节点 ID 和数据 ID 越大，表示数据越新，优先成为主。相比较于 Raft 算法，ZAB 算法尽可能保证数据的最新性。所以，ZAB 算法可以说是对 Raft 算法的改进\n状态 说明 Looking 选举 当节点处于该状态时，它会认为当前集群中没有 Leader，因此自己进入选举状态 Leading 领导 表示已经选出主，且当前节点为 Leader Following 跟随 集群中已经选出主后，其他非主节点状态更新为 Following，表示对 Leader 的追随 Observing 观察 表示当前节点为 Observer，持观望态度，没有投票权和选举权 核心\n少数服从多数，ID 大的节点优先成为主 ZAB 算法性能高，对系统无特殊要求，采用广播方式发送信息，若节点中有 n 个节点，每个节点同时广播，则集群中信息量为 n*(n-1) 个消息，容易出现广播风暴；\n且除了投票，还增加了对比节点 ID 和数据 ID，这就意味着还需要知道所有节点的 ID 和数据 ID，所以选举时间相对较长。\n但该算法选举稳定性比较好，当有新节点加入或节点故障恢复后，会触发选主，但不一定会真正切主，除非新节点或故障后恢复的节点数据 ID 和节点 ID 最大，且获得投票数过半，才会导致切主。\n选举对比 # Bully Raft ZAB 选举原则 节点ID最大 投票最多 数据最新或ID最大 优点 容易理解,选举速度快,算法复杂度低,易于实现 选举速度快,算法复杂度低,易于实现 性能高 缺点 信心存储量大,易频繁切主,不适大规模 要求系统全连接,消息通信量大,不适大规模 选举时间长,复杂度高 应用场景 适合小规模,MongoDB 适合中小规模,K8S集群中3个节点选举 适合中大规模,Zookeeper 分布式共识 # 分布式共识就是在多个节点均可独自操作或记录的情况下，使得所有节点针对某个状态达成一致的过程\n下文以区块链技术共识机制举例.\n解决方式\nPoW // Proof-of-Work，工作量证明 PoS // Proof-of-Stake，权益证明 DPoS // Delegated Proof of Stake，委托权益证明 分布式共识核心\n记账权 所有节点或服务器达成一致 PoW # 是以每个节点或服务器的计算能力（即“算力”）来竞争记账权的机制，因此是一种使用工作量证明机制的共识算法\nPoW 的容错机制,允许全网 50% 的节点出错.缺点是共识达成的周期长、效率低，资源消耗大.\n共识的时间影响因素\nPoW 机制每次达成共识需要全网共同参与运算，增加了每个节点的计算量. 如果题目过难，会导致计算时间长、资源消耗多. 如果题目过于简单，会导致大量节点同时获得记账权，冲突多. PoS # 核心原理:由系统权益代替算力来决定区块记账权，拥有的权益越大获得记账权的概率就越大\n缩短了达成共识时间,但容易出现垄断现象.\nDPoS # DPoS 算法的原理，类似股份制公司的董事会制度，普通股民虽然拥有股权，但进不了董事会，他们可以投票选举代表（受托人）代他们做决策。DPoS 是由被社区选举的可信帐户（受托人，比如得票数排行前 101 位）来拥有记账权。\n缺点:\n持币人投票积极性不高. 故障问题解决效率低,易出现安全隐患. 拜占庭将军问题 # 假设一支军队由多个将军组成，这些将军需要就进攻或撤退的决策达成共识。然而，一些将军可能是不可信的，他们可能发送虚假的消息或者完全拒绝发送消息。此外，将军之间的通信可能会受到敌人的干扰，导致消息被篡改或丢失\n人数比例\n如果叛将人数为 m，将军人数不能少于 3m + 1 ，那么拜占庭将军问题就能解决了. The Byzantine Generals Problem by Leslie Lamport\n消息签名机制防伪\n分布式锁 # 锁是实现多线程同时访问同一共享资源，保证同一时刻只有一个线程可访问共享资源所做的一种标记\n主流方法\n关系型数据库实现分布式锁 // 适用于并发量低，对性能要求低的场景 cache实现分布式锁 // 利用SetNX原子操作,但锁失效时间控制不稳定 ZooKeeper实现分布式锁 // 可靠性最高 "},{"id":2,"href":"/coding/docs/distributed/dt/","title":"02.1 分布式事务","section":"分布式","content":" 简介 # 分布式事务，就是在分布式系统中运行的事务，由多个本地事务组合而成\n方案\n基于 XA 协议的二阶段提交协议方法 三阶段提交协议方法 基于消息的最终一致性方法 XA二阶段提交 # 二阶段提交: Two-Phase Commit，2PC\nXA(Extended Architecture)是一个分布式事务协议\n涉及对象\n事务管理器 // 事务协调者Transaction Coordinator,负责各个本地资源的提交和回滚 本地资源管理器 // 分布式事务的参与者Participants,执行实际的操作,如数据库或其他资源 执行过程\n投票（voting） 提交（commit） 举例\n第一阶段 sequenceDiagram participant 协调者 participant 订单系统 participant 库存系统 协调者-\u003e\u003e订单系统: 询问订单情况 订单系统--\u003e\u003e协调者: 锁定用户A相关订单,增加一条购买100件T恤的订单 订单系统--\u003e\u003e协调者: 回复同意消息\"Yes\" 协调者-\u003e\u003e库存系统: 询问出货情况 库存系统--\u003e\u003e协调者: 回复库存不足信息\"No\" 第二阶段 sequenceDiagram participant 协调者 participant 订单系统 participant 库存系统 协调者-\u003e\u003e订单系统: 发送\"DoAbort\"消息 订单系统--\u003e\u003e协调者: 回复\"HaveCommitted\"消息 协调者-\u003e\u003e库存系统: 发送\"DoAbort\"消息 库存系统--\u003e\u003e协调者: 回复\"HaveCommitted\"消息 缺点 # 同步阻塞问题 单点故障问题 数据不一致问题 三阶段提交方法 # 三阶段提交协议（Three-phase commit protocol，3PC）,三阶段提交引入了超时机制和准备阶段\nCanCommit PreCommit // 因为超时或条件不充分进行快速失败 DoCommit 消息最终一致性 # 将需要分布式处理的事务通过消息或者日志的方式异步执行，消息或日志可以存到本地文件、数据库或消息队列中，再通过业务规则进行失败重试\n是BASE理论体现,牺牲了强一致性采取最终一致性.\n基于 MQ 的消息投递 # 实现生产端和消费端的双向确认\nMQ 自动应答机制导致的消息丢失 采取编程的方式手动发送应答 高并发场景下的消息积压导致消息丢失 未完成的消息重新投递来进行消息补偿 "},{"id":3,"href":"/coding/docs/distributed/resource/","title":"03. 资源与负载","section":"分布式","content":" 极客时间.分布式技术原理与算法解析 笔记\n分布式体系结构 # 集中式结构 # Google Borg Kubernetes Mesos 非集中式结构 # Akka 集群 Redis 集群 Cassandra 集群 调度 # 而为用户任务寻找合适的服务器这个过程，在分布式领域中叫作调度.\n调度是以任务为单位的，而不是以作业为单位.\n单体调度 # 是由一个中央调度器去管理整个集群的资源信息和任务调度，也就是说所有任务只能通过中央调度器进行调度\nBorg 调度算法\n可行性检查，找到一组可以运行任务的机器（Borglet） 评分，从可行的机器中选择一个合适的机器（Borglet） 最差匹配 最佳匹配 两层调度 # 是将资源管理和任务调度分为两层来调度。\n第一层调度器负责集群资源管理，并将可用资源发送给第二层调度 第二层调度接收到第一层调度发送的资源，进行任务调度 共享状态调度 # 多个调度器，每个调度器都可以看到集群的全局资源信息，并根据这些信息进行任务调度\n分布式计算 # MapReduce // 分而治之 Stream // 实时 Actor Erlang/OTP Akka Quasar (Java) Pipeline 分布式通信 # RPC Pub/Sub 消息队列 "},{"id":4,"href":"/coding/docs/distributed/ds/","title":"03.1 分布式存储","section":"分布式","content":" CAP # CA CP AP 场景 单机 强一致性,金融银行 及时响应,容忍一致性,商品查询 应用 Mysql Redis,Hbase,ZooKeeper CoachDB,Cassandra,DynamoDB 数据分片 # 数据分片即按照一定的规则将数据路由到相应的存储节点中，从而降低单存储节点带来的读写压力\n哈希分片 根据key的哈希值来决定分配到哪个分片 不依赖key的顺序,只依赖哈希函数 范围分片 根据key的大小顺序来决定属于哪个分片范围 依赖于key有明确的大小顺序 一致性哈希环 将节点和数据项映射到一个虚拟圆环空间上 通过这种映射来实现分片和负载均衡 对节点添加和删除影响较小 一致性哈希环 # hash算法的缺陷 好刚: 7分钟视频详解一致性hash 算法\n以图片服务器举例,我们希望图片均匀落在不同的图片服务器上.\n以图片名为key,hash后再根据机器数取模,在此规则下.\n假设图片hash=6,机器数为3, %3=0,则图片放置在0号服务器.\n此时,我们增加1台机器,机器数为4,图片hash依旧为6,%4=2,则映射到了2号服务器,\n此时原本在0号服务器的图片,去了2号服务器查找不存在,又要通过后端服务查找一遍,可能导致缓存雪崩.\n一致性哈希环 则可以减少数据失效程度. hash偏斜与虚拟节点 数据结构分类 # 分布式数据库\nMySQL Sharding Microsoft SQL Azure Google Spanner Alibaba OceanBase KV数据库\nRedis Memcache 分布式存储系统\nCeph GFS HDFS Swift 数据一致性 # 强一致性 数据一致性 Raft 协议 Gossip 协议 Gossip # 分布式原理：10分钟带你全面了解Gossip协议！\nGossip 的协议原理有一种传播机制叫谣言传播，指的是当一个节点有了新数据后，这个节点就变成了活跃状态，并周期性地向其他节点发送新数据，直到所有的节点都存储了该条数据。这种方式达成的数据一致性是 “最终一致性”，即执行数据更新操作后，经过一定的时间，集群内各个节点所存储的数据最终会达成一致，很适合动态变化的分布式系统。\n如果副本多、参与共识的节点多，那就更适合采用 Gossip 这种最终一致性协议\n"},{"id":5,"href":"/coding/docs/mysql/b_tree/","title":"b + 树","section":"mysql","content":" 为什么 MySQL 采用 B+ 树作为索引?\n树结构的对比种类 # 二叉查找树 平衡二叉查找树 B tree B + tree 二叉查找树 # 二叉查找树(Binary Search Tree)的特点是一个节点的左子树的所有节点都小于这个节点，右子树的所有节点都大于这个节点\n查询和插入删除效率较高,但平衡性不佳,最坏情况下时间复杂度为O(n)\n当每次插入的元素都是二叉查找树中最大的元素，二叉查找树就会退化成了一条链表，查找数据的时间复杂度变成了 O(n)\n遍历 # 先序遍历按照“根节点-\u0026gt;左子树-\u0026gt;右子树”的顺序进行遍历 中序遍历按照“左子树-\u0026gt;根节点-\u0026gt;右子树”的顺序进行遍历 后序遍历按照“左子树-\u0026gt;右子树-\u0026gt;根结点”的顺序进行遍历 平衡二叉查找树 # AVL树为实现平衡二叉查找树的数据结构\n每个节点的左子树和右子树的高度差不能超过 1\n时间复杂度降低到O(log n)\n不管平衡二叉查找树还是红黑树，都会随着插入的元素增多，而导致树的高度变高，这就意味着磁盘 I/O 操作次数多，会影响整体数据查询的效率\nB tree # 当树的节点越多的时候，并且树的分叉数 M 越大的时候，M 叉树的高度会远小于二叉树的高度\nB + tree # B+ 树的非叶子节点不存放实际的记录数据，仅存放索引，因此数据量相同的情况下，相比存储即存索引又存记录的 B 树，B+树的非叶子节点可以存放更多的索引，因此 B+ 树可以比 B 树更「矮胖」，查询底层节点的磁盘 I/O次数会更少\nB+ 树所有叶子节点间还有一个链表进行连接，这种设计对范围查找非常有帮助\n"},{"id":6,"href":"/coding/docs/distributed/distributed/","title":"BASE理论","section":"分布式","content":" CAP原则 # CAP原则（CAP theorem）是一个分布式系统理论，它指出在一个分布式计算系统中，无法同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三个目标。\n一致性（Consistency） # 要求系统中的所有节点在同一时间具有相同的数据副本，即读操作应该总是返回最新的写操作结果。\n可用性（Availability） # 要求系统在任何时间都能够提供响应，即系统不会因为部分节点的故障而导致整个系统不可用。\n分区容错性（Partition tolerance） # 要求系统能够继续运行，即使系统内的节点因为网络问题而无法互相通信。\n根据CAP原则，当一个分布式系统发生网络分区时，为了保证系统的可用性和分区容错性，必须选择放弃一致性。这意味着在网络分区的情况下，系统可以选择提供最新的数据副本（追求一致性），但这可能导致某些节点无法访问；或者系统可以继续提供访问服务（追求可用性），但可能会返回不一致的数据。\n需要注意的是，CAP原则并不是指分布式系统必须选择放弃一致性，而是在面临网络分区时需要做出权衡。不同的分布式系统可能会根据具体需求和设计目标，在一致性、可用性和分区容错性之间作出不同的选择。\nBASE理论 # BASE理论是对CAP原则的一种实践指导，它是对传统ACID（原子性、一致性、隔离性和持久性）事务模型的一种松散的替代理论。BASE代表着基本可用（Basically Available）、软状态（Soft State）和最终一致性（Eventually Consistent）。\n基本可用（Basically Available） # 系统保证在面对部分故障或者分区的情况下，仍能够提供基本的可用性和服务能力，即系统能够对用户的请求做出响应，尽管可能会有部分功能受限或者性能下降。\n软状态（Soft State） # 系统中的数据状态不需要实时保持一致，允许在一段时间内存在不同节点之间的数据副本不一致的情况。这意味着系统可以容忍一定的数据冗余和延迟，以换取更高的可用性和性能。\n最终一致性（Eventually Consistent） # 系统会在一段时间内尽可能地使数据达到一致状态，但并不要求实时保证一致性。系统允许在数据复制和同步过程中存在一定的延迟和不一致，但最终会达到一致的状态。\nBASE理论的核心思想是放宽对一致性的要求，以换取更高的可用性、性能和分布式容错性。相比于ACID事务模型的强一致性和事务隔离性，BASE理论提供了更灵活的设计选择，特别适用于大规模分布式系统和互联网应用场景。在BASE理论下，系统设计者需要根据具体的业务需求和系统特点，权衡一致性与可用性之间的取舍，并选择适当的一致性模型和数据同步策略。\n幂等 # 任意次执行都会产生相同的结果：无论操作执行多少次，结果都是相同的，不会受到重复执行的影响。 重复执行不会引起不良影响：对于已经执行过的操作，重复执行不会引起任何不良的副作用或状态变化。 "},{"id":7,"href":"/coding/docs/go/chan/","title":"chan","section":"golang","content":" 操作chan # 创建chan\nmake(chan T, [capacity]) // 无缓冲,创建一个int类型的channel ch := make(chan int) // 有缓冲,创建一个容量为5的int缓冲channel ch := make(chan int, 5) 收发chan\nch \u0026lt;- elem // 发送 elem := \u0026lt;-ch //接收 // 如果ok为false,那么可能是channel已经关闭,读到的时关闭后的零值。 i, ok \u0026lt;- ch panic 情况 # 向关闭chan发送 关闭nil chan 关闭已关闭 chan 操作 nil chan 已关闭chan 正常chan 关闭 panic panic 正常关闭 读 阻塞 读可以继续读取元素,直到chan空,如果读完会读到对应类型的零值 读会阻塞,如果chan空或者没有其他goroutine写入 写 阻塞 panic 写也可能阻塞,如果chan空间不足或者没有其他goroutine读取 数据结构 # type hchan struct { qcount uint // 队列中的元素总数量 dataqsiz uint // 循环队列的长度 buf unsafe.Pointer // 指向长度为 dataqsiz 的底层数组 elemsize uint16 // 能够接受和发送的元素大小 closed uint32 // 是否关闭 elemtype *_type // 能够接受和发送的元素类型 sendx uint // 已发送元素在循环队列中的索引位置 recvx uint // 已接收元素在循环队列中的索引位置 recvq waitq // 接受者的 sudog 等待队列 sendq waitq // 发送者的 sudog 等待队列 // lock 保护 hchan 中的所有字段，以及 blocked on 此 channel 的 sudog 中的一些字段。 // // 在持有此锁时不要更改另一个 G 的状态（特别是不要准备一个 G）， // 因为这可能会导致与栈缩减死锁。 lock mutex } "},{"id":8,"href":"/coding/docs/mysql/count/","title":"count","section":"mysql","content":" count(1) and count(*) # count(*)包括了所有的列，相当于行数，在统计结果的时候，不会忽略NULL\ncount(1) and count(列名) # count(1) 会统计表中的所有的记录数，不会忽略NULL，包含字段为null 的记录 count(列名) 会统计该列字段在表中出现的次数，会忽略字段为null 的情况，即不统计字段为null 的记录 执行效率 # 若列名为主键，count(列名)会比count(1)快 若列名不为主键，count(1)会比count(列名)快 若表多个列并且没有主键，则 count（1） 的执行效率优于 count（*） 若表有主键，则 select count（主键）的执行效率是最优的 若表只有一个字段，则 select count（*）最优。 优先使用count(*)或count(主键id);若主键不能用,则用count(1);若都不行才使用count(字段)\n如何优化 count(*)？ # 近似值,explain 命令来表进行估算 额外表保存计数值 "},{"id":9,"href":"/coding/docs/mysql/ddl/","title":"ddl","section":"mysql","content":" DDL # MySQL DDL(Data Definition Language)的是MySQL数据库的数据定义语言,用于定义和管理数据库对象,如数据库、表、列等。\n主要的MySQL DDL语句包括:\nCREATE:用于创建数据库对象,如创建数据库、表等。比如CREATE DATABASE、CREATE TABLE。\nALTER:用于修改数据库对象的结构或属性,如ALTER TABLE修改表结构。\nDROP:用于删除数据库对象,如DROP DATABASE删除数据库,DROP TABLE删除表。\nTRUNCATE:清空表中的所有行而不删除表本身。\nRENAME:用来重命名数据库对象,如RENAME TABLE重命名表。\nCOMMENT:为数据库对象添加注释,如COMMENT ON TABLE为表添加注释。\nINDEX:创建和删除数据库索引,如CREATE INDEX添加索引,DROP INDEX删除索引。\n其他类似 # DML(Data Manipulation Language) # 数据操作语言,用于对数据记录进行增删改操作,比如INSERT、UPDATE、DELETE等语句。\nDQL(Data Query Language) # 数据查询语言,用于对数据库进行选择和查询,主要是SELECT语句。\nDCL(Data Control Language) # 数据控制语言,用于控制数据库访问权限,如GRANT和REVOKE语句。\nTCL(Transaction Control Language) # 事务控制语言,用于管理数据库事务,如COMMIT、ROLLBACK、SAVEPOINT语句。\nDAL(Data Analysis Language) # 数据分析语言,用于对数据库进行统计和分析计算,如COUNT、SUM、AVG函数等。\nDSL(Data Definition Language) # 数据定义语言,用于定义数据库索引与视图等,如CREATE INDEX和CREATE VIEW语句。\n"},{"id":10,"href":"/coding/docs/go/defer/","title":"defer","section":"golang","content":" 流程 # 返回值 = xxx\n调用defer函数\n空的return\n"},{"id":11,"href":"/coding/docs/mysql/delete/","title":"delete与truncate","section":"mysql","content":"速度:drop \u0026gt; truncate \u0026gt; DELETE\nDML:delete DDL:drop,truncate\nDELETE:删除表中的某些行,但不删除表结构。删除后数据行数减少,但表还存在。 TRUNCATE:清空表内所有数据,但不删除表结构。与DELETE不同的是,TRUNCATE没有事务日志,效率更高。 DROP:完全删除表,表结构和表数据一起删除。删除后无法恢复。 可以这么理解，一本书，delete是把目录撕了，truncate是把书的内容撕下来烧了，drop是把书烧了\n"},{"id":12,"href":"/coding/docs/distributed/etcd/","title":"etcd","section":"分布式","content":" 极客时间.etcd 实战课 笔记\n历史 # 背景: CoreOS 团队需要一个协调服务来存储服务配置信息、提供分布式锁等能力\n服务所需目标:\n高可用 数据一致,提供读取\u0026quot;最新\u0026quot;数据 低容量、仅存储关键元数据配置 增删改查，监听数据变化的机制 可维护性 名字来源: unix /etc + d of distribute\n历史版本变化\nv0.1 # Raft算法共识 REST API 数据模型使用的是基于目录的层次模式// 参考ZooKeeper key-value 存储引擎上,简单内存树 Go语言 v0.2 # 支持consistent read CAS提供原子性 //替换掉Test And Set 机制 v2.0 # 支持quorum read v3 # 引入 B-tree,boltdb 实现一个 MVCC 数据库 数据模型从层次型目录结构改成扁平的 key-value gRPC+protobuf 特性\n提供稳定可靠的事件通知 实现了事务 支持多 key 原子更新 同时基于 boltdb 的持久化存储，显著降低了 etcd 的内存占用、避免了 etcd v2 定期生成快照时的昂贵的资源开销.\n使用了 gRPC API，使用 protobuf 定义消息，消息编解码性能相比 JSON 超过 2 倍以上，并通过 HTTP/2.0 多路复用机制，减少了大量 watcher 等场景下的连接数.\n其次使用 Lease 优化 TTL 机制，每个 Lease 具有一个 TTL，相同的 TTL 的 key 关联一个 Lease，Lease 过期的时候自动删除相关联的所有 key，不再需要为每个 key 单独续期.\n最后是 etcd v3 支持范围、分页查询，可避免大包等 expensive request.\n通过历史了解到了etcd的特性与功能 概念 # quorum机制 # 比如node数量为10,我们写入3个成功,那么读取则必须为8个以上,再根据节点的最新数据获取,读才能一致.\nquorum的读写最小票数可以用来做为系统在读、写性能方面的一个可调节参数。写票数Vw越大，则读票数Vr越小，这时候系统读的开销就小。反之则写的开销就小。\nbolt # bolt是一个key-value数据库.\nLease # Lease即租约意思.\nLease 机制是一种分布式系统中常用的协作机制，用于控制对共享资源的访问。它基于一种简单的想法：将资源的控制权租借给一个实体，以允许该实体在一段时间内独占访问资源。Lease 机制通常包括以下关键元素：\n租约持有者（Lease Holder）:一个实体，通常是一个进程或节点，持有资源的租约。只有租约持有者才能访问资源。\n租约超时时间（Lease Timeout）:租约被授予的时间期限。一旦租约超时，资源将被释放，其他实体可以获得租约。\n租约续约（Lease Renewal）:租约持有者可以在租约即将到期时请求续约，以延长其对资源的访问权限。\nLease 机制的主要目标是确保资源的独占性和一致性。通过将资源租借给一个实体，系统可以避免多个实体同时访问资源而导致的竞态条件和数据不一致性问题。\n读场景 # 串行读 # 如下图所示，当 client 发起一个更新 hello 为 world 请求后，若 Leader 收到写请求，它会将此请求持久化到 WAL 日志，并广播给各个节点，若一半以上节点持久化成功，则该请求对应的日志条目被标识为已提交，etcdserver 模块异步从 Raft 模块获取已提交的日志条目，应用到状态机 (boltdb 等)\nWAL(write-ahead log) 此时若 client 发起一个读取 hello 的请求，假设此请求直接从状态机中读取， 如果连接到的是 C 节点，若 C 节点磁盘 I/O 出现波动，可能导致它应用已提交的日志条目很慢，则会出现更新 hello 为 world 的写命令，在 client 读 hello 的时候还未被提交到状态机，因此就可能读取到旧数据，如上图查询 hello 流程所示。\n适合低延时、高吞吐量,对数据一致性要求不高. 线性读 # ReadIndex 当收到一个线性读请求时，它首先会从 Leader 获取集群最新的已提交的日志索引 (committed index)， 如上图中的流程二所示.\nLeader 收到 ReadIndex 请求时，为防止脑裂等异常场景，会向 Follower 节点发送心跳确认， 一半以上节点确认 Leader 身份后才能将已提交的索引 (committed index) 返回给节点 C(上图中的流程三).\nC 节点则会等待，直到状态机已应用索引 (applied index) 大于等于 Leader 的已提交索引时 (committed Index)(上图中的流程四)， 然后去通知读请求，数据已赶上 Leader，你可以去状态机中访问数据了 (上图中的流程五). 适合对数据一致性要求高. MVCC # MVCC: 解决etcd v2 不支持保存 key 的历史版本、不支持多 key 事务等问题而产生的. 核心组成\n内存树形索引模块 (treeIndex) 嵌入式的 KV 持久化存储库 boltdb boltdb # boltdb 保存一个 key 的多个历史版本,方案选择:\n一个 key 保存多个历史版本的值// []struct 每次修改操作，生成一个新的版本号 (revision)，以版本号为 key， value 为用户 key-value 等信息组成的结构体 // struct 后者是etcd采用方案.\n读事务 # treeIndex 与 boltdb 关系如下面的读事务流程图所示 etcd 在执行读请求过程中涉及磁盘 IO 吗? etcd在启动的时候会通过mmap机制将etcd db文件映射到etcd进程地址空间，并设置了mmap的MAP_POPULATE flag， 它会告诉Linux内核预读文件，Linux内核会将文件内容拷贝到物理内存中，此时会产生磁盘I/O。节点内存足够的请求下，后续处理读请求过程中就不会产生磁盘I/IO了。\n若etcd节点内存不足，可能会导致db文件对应的内存页被换出，当读请求命中的页未在内存中时，就会产生缺页异常，导致读过程中产生磁盘IO，你可以通过观察etcd进程的majflt字段来判断etcd是否产生了主缺页中断\n写场景 # Quota # 配额（Quota）模块\netcd db 文件大小超过了配额\n\u0026ldquo;etcdserver: mvcc: database space exceeded\u0026rdquo;\n它是指当前 etcd db 文件大小超过了配额，当出现此错误后，你的整个集群将不可写入，只读，对业务的影响非常大。\nQuota 工作流程\n当 etcd server 收到 put/txn 等写请求的时候，会首先检查下当前 etcd db 大小加上你请求的 key-value 大小之和是否超过了配额（quota-backend-bytes）。\n如果超过了配额，它会产生一个告警（Alarm）请求，告警类型是 NO SPACE，并通过 Raft 日志同步给其它节点，告知 db 无空间了，并将告警持久化存储到 db 中\n最终，无论是 API 层 gRPC 模块还是负责将 Raft 侧已提交的日志条目应用到状态机的 Apply 模块，都拒绝写入，集群只读\n解决\n调大配额,etcd 社区建议不超过 8G. 额外发送一个取消告警（etcdctl alarm disarm）的命令，以消除所有告警,否则集群依然拒绝写入. 检查 etcd 的压缩（compact）配置是否开启、配置是否合理 Preflight Check # 为了保证集群稳定性，避免雪崩，任何提交到 Raft 模块的请求，都会做一些简单的限速判断\n如果 Raft 模块已提交的日志索引（committed index）比已应用到状态机的日志索引（applied index）超过了 5000，那么它就返回一个\u0026quot;etcdserver: too many requests\u0026ldquo;错误给 client。\n然后它会尝试去获取请求中的鉴权信息，若使用了密码鉴权、请求中携带了 token，如果 token 无效，则返回\u0026rdquo;auth: invalid auth token\u0026ldquo;错误给 client。\n其次它会检查你写入的包大小是否超过默认的 1.5MB， 如果超过了会返回\u0026rdquo;etcdserver: request is too large\u0026ldquo;错误给给 client。\nPropose # 最后通过一系列检查之后，会生成一个唯一的ID，将此请求关联到一个对应的消息通知 channel，然后向 Raft 模块发起（Propose）一个提案（Proposal），提案内容为“大家好，请使用 put 方法执行一个 key 为 hello，value 为 world 的命令”，也就是整体架构图里的流程四。\n向 Raft 模块发起提案后，KVServer 模块会等待此 put 请求，等待写入结果通过消息通知 channel 返回或者超时。 etcd 默认超时时间是 7 秒（5 秒磁盘 IO 延时 +2*1 秒竞选超时时间）， 如果一个请求超时未返回结果，则可能会出现你熟悉的 etcdserver: request timed out 错误。\nWAL # Raft 模块收到提案后，如果当前节点是 Follower，它会转发给 Leader，只有 Leader 才能处理写请求。Leader 收到提案后， 通过 Raft 模块输出待转发给 Follower 节点的消息和待持久化的日志条目，日志条目则封装了我们上面所说的 put hello 提案内容。\netcdserver 从 Raft 模块获取到以上消息和日志条目后，作为 Leader，它会将 put 提案消息广播给集群各个节点， 同时需要把集群 Leader 任期号、投票信息、已提交索引、提案内容持久化到一个 WAL（Write Ahead Log）日志文件中，用于保证集群的一致性、可恢复性，也就是我们图中的流程五模块。\nWAL日志结构\n上图是 WAL 结构，它由多种类型的 WAL 记录顺序追加写入组成， 每个记录由类型、数据、循环冗余校验码组成。 同类型的记录通过 Type 字段区分，Data 为对应记录内容，CRC 为循环校验码信息。\nWAL 记录类型目前支持 5 种，分别是文件元数据记录、日志条目记录、状态信息记录、CRC 记录、快照记录：\n文件元数据记录包含节点 ID、集群 ID 信息，它在 WAL 文件创建的时候写入； 日志条目记录包含 Raft 日志信息，如 put 提案内容； 状态信息记录，包含集群的任期号、节点投票信息等，一个日志文件中会有多条，以最后的记录为准； CRC记录包含上一个 WAL 文件的最后的 CRC（循环冗余校验码）信息， 在创建、切割 WAL 文件时，作为第一条记录写入到新的 WAL 文件， 用于校验数据文件的完整性、准确性等； 快照记录包含快照的任期号、日志索引信息，用于检查快照文件的准确性。 日志条目有经过数据格式封装,fsync 持久化到磁盘\n当一半以上节点持久化此日志条目后， Raft 模块就会通过 channel 告知 etcdserver 模块， put 提案已经被集群多数节点确认，提案状态为已提交，你可以执行此提案内容了\n于是进入流程六，etcdserver 模块从 channel 取出提案内容，添加到先进先出（FIFO）调度队列，随后通过 Apply 模块按入队顺序，异步、依次执行提案内容。\nApply # crash应对\netcd 重启时，会从 WAL 中解析出 Raft 日志条目内容，追加到 Raft 日志的存储中，并重放已提交的日志提案给 Apply 模块执行\n幂等保证\n唯一的字段能标识这个提案是Raft 日志条目中的索引（index）字段.\n日志条目索引是全局单调递增的，每个日志条目索引对应一个提案， 如果一个命令执行后， 我们在 db 里面也记录下当前已经执行过的日志条目索引，是不是就可以解决幂等性问题呢？\n是的。但是这还不够安全，如果执行命令的请求更新成功了，更新 index 的请求却失败了，是不是一样会导致异常？\n因此我们在实现上，还需要将两个操作作为原子性事务提交，才能实现幂等。\n正如我们上面的讨论的这样，etcd 通过引入一个 consistent index 的字段，来存储系统当前已经执行过的日志条目索引，实现幂等性。\nMVCC # 内存索引模块 treeIndex，保存 key 的历史版本号信息 boltdb 模块，用来持久化存储 key-value 数据 treeIndex\n版本号（revision）在 etcd 里面发挥着重大作用，它是 etcd 的逻辑时钟。 etcd 启动的时候默认版本号是 1，随着你对 key 的增、删、改操作而全局单调递增\nboltdb\n写入 boltdb 的 value\n为了构建索引和支持 Lease 等特性\nkey 名称 key 创建时的版本号（create_revision）、最后一次修改时的版本号（mod_revision）、key 自身修改的次数（version） value 值 租约信息 事务提交的过程，包含 B+tree 的平衡、分裂，将 boltdb 的脏数据（dirty page）、元数据信息刷新到磁盘， 因此事务提交的开销是昂贵的。 如果我们每次更新都提交事务，etcd 写性能就会较差。\n合并再合并\n首先 boltdb key 是版本号，put/delete 操作时，都会基于当前版本号递增生成新的版本号，因此属于顺序写入，可以调整 boltdb 的 bucket.FillPercent 参数，使每个 page 填充更多数据， 减少 page 的分裂次数并降低 db 空间。\n其次 etcd 通过合并多个写事务请求，通常情况下，是异步机制定时（默认每隔 100ms）将批量事务一次性提交（pending 事务过多才会触发同步提交）， 从而大大提高吞吐量，对应上面简易写事务图的流程三。\n但是这优化又引发了另外的一个问题， 因为事务未提交，读请求可能无法从 boltdb 获取到最新数据。\n为了解决这个问题，etcd 引入了一个 bucket buffer 来保存暂未提交的事务数据。在更新 boltdb 的时候，etcd 也会同步数据到 bucket buffer。因此 etcd 处理读请求的时候会优先从 bucket buffer 里面读取，其次再从 boltdb 读，通过 bucket buffer 实现读写性能提升，同时保证数据一致性。\nexpensive request是否影响写请求性能 在etcd 3.0中，线性读请求需要走一遍Raft协议持久化到WAL日志中，因此读性能非常差，写请求肯定也会被影响。\n在etcd 3.1中，引入了ReadIndex机制提升读性能，读请求无需再持久化到WAL中。\n在etcd 3.2中, 优化思路转移到了MVCC/boltdb模块，boltdb的事务锁由粗粒度的互斥锁，优化成读写锁，实现“N reads or 1 write”的并行，同时引入了buffer来提升吞吐量。问题就出在这个buffer，读事务会加读锁，写事务结束时要升级锁更新buffer，但是expensive request导致读事务长时间持有锁，最终导致写请求超时。\n在etcd 3.4中，实现了全并发读，创建读事务的时候会全量拷贝buffer, 读写事务不再因为buffer阻塞，大大缓解了expensive request对etcd性能的影响。尤其是Kubernetes List Pod等资源场景来说，etcd稳定性显著提升.\netcd数据不一致的风险 # 最佳实践\n开启etcd的数据毁坏检测功能； 应用层的数据一致性检测； 定时数据备份； 良好的运维规范（比如使用较新稳定版本、确保版本一致性、灰度变更） "},{"id":13,"href":"/coding/docs/go/gc/","title":"gc","section":"golang","content":" 标记-清除算法 # STW(stop the world) 让程序暂停，程序出现卡顿\n标记(Mark phase) 清除(Sweep phase) 缺点:\nSTW，stop the world；让程序暂停，程序出现卡顿 (重要问题)； 标记需要扫描整个heap； 清除数据会产生heap碎片。 三色并发标记法 # 说明 # 三种颜色:白色( White)、灰色(Grey)、黑色(Black)\n白色:对象尚未被访问过 灰色:对象正在被访问 黑色:对象访问已完成 工作流程\n开始时所有对象都为白色 根集对象(比如静态变量等)被标记为灰色 递归地遍历从根集对象开始的可达对象,将其标记为灰色 从灰色队列中取出一个对象,访问其字段和引用,将被访问的对象标记为灰色 对灰色队列中的对象进行同样操作,直到队列为空 将遍历完的灰色对象标记为黑色 从第二步开始重复遍历过程,直到没有灰色对象为止 遍历结束后,扫描内存,回收尚为白色的未使用对象 白-\u0026gt;灰-\u0026gt;黑\n第一步 , 每次新创建的对象，默认的颜色都是标记为\u0026quot;白色\u0026quot; 第二步, 每次GC回收开始, 会从根节点开始遍历所有对象，把遍历到的对象从白色集合放入“灰色” 第三步, 遍历灰色集合，将灰色对象引用的对象从白色集合放入灰色集合，之后将此灰色对象放入黑色集合 为了在GC过程中保证数据的安全，我们在开始三色标记之前就会加上STW，在扫描确定黑白对象之后再放开STW\n没有STW加持下存在的问题 # 条件1: 一个白色对象被黑色对象引用(白色被挂在黑色下) 条件2: 灰色对象与它之间的可达关系的白色对象遭到破坏(灰色同时丢了该白色) 如果当以上两个条件同时满足时，就会出现对象丢失现象!\n屏障机制 # 三色不变式\n强三色不变式 不存在黑色对象引用到白色对象的指针 弱三色不变式 所有被黑色对象引用的白色对象都处于灰色保护状态。 插入屏障 # 在A对象引用B对象的时候，B对象被标记为灰色\n删除屏障 # 被删除的对象，如果自身为灰色或者白色，那么被标记为灰色\n缺点\n插入写屏障：结束时需要STW来重新扫描栈，标记栈上引用的白色对象的存活； 删除写屏障：回收精度低，GC开始时STW扫描堆栈来记录初始快照，这个过程会保护开始时刻的所有存活对象。 混合写屏障 # GC开始将栈上的对象全部扫描并标记为黑色(之后不再进行第二次重复扫描，无需STW)， GC期间，任何在栈上创建的新对象，均为黑色。 被删除的对象标记为灰色。 被添加的对象标记为灰色。 "},{"id":14,"href":"/coding/docs/ms/framework/","title":"Go框架","section":"微服务","content":" 简介 # https://www.techempower.com/benchmarks/\nweb框架 # gin doc github echo doc github hertz doc github star-history\n数据库框架 # ent doc github gorm doc github star-history\n微服务框架 # 来源 go-kratos doc github bilibili kitex doc github 字节跳动 go-zero doc github 晓黑板 TarsGo github 腾讯 star-history\n"},{"id":15,"href":"/coding/docs/go/interface/","title":"interface","section":"golang","content":" 数据结构 # // 结构体表示包含方法的接口 type iface struct { tab *itab data unsafe.Pointer } // 结构体表示不包含任何方法的 interface{} 类型 type eface struct { _type *_type data unsafe.Pointer } Golang中的Interface可以被看作是一个Wrapper，它是一个包含了value和type的二元组\n// 必须类型和值都为nil才算真正的nil var a interface{} = nil // tab = nil, data = nil var b interface{} = (*int)(nil) // tab 包含 *int 类型信息, data = nil fmt.Println(a == nil) // true fmt.Println(b == nil) // false 判断动态值为nil # func IsNil(i interface{}) bool { vi := reflect.ValueOf(i) if vi.Kind() == reflect.Ptr { return vi.IsNil() } return false } 用处 # // 编译器会由此检查 *myWriter 类型是否实现了 io.Writer 接口 var _ io.Writer = (*myWriter)(nil) "},{"id":16,"href":"/coding/docs/ms/kubernetes/","title":"k8s","section":"微服务","content":" 极客时间.深入剖析 Kubernetes\n容器基础 # 容器技术的核心功能，就是通过约束和修改进程的动态表现，从而为其创造出一个\u0026quot;边界\u0026quot;.\nCgroups 技术// 制造约束 Namespace 技术//修改进程视图 Namespace # 用来对各种不同的进程上下文进行“障眼法”操作\nPID Mount UTS IPC Network User Cgroup // Linux 内核从 4.6 开始 PID namespace # 对被隔离应用的进程空间做了手脚，使得这些进程只能看到重新计算过的进程编号，比如 PID=1。可实际上，他们在宿主机的操作系统里，还是原来的第 100 号进程\nint pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL); 多次执行上面的 clone(),就会创建多个 PID Namespace,每个 Namespace 里的应用进程，都会认为自己是当前容器里的第 1 号进程，它们既看不到宿主机里真正的进程空间，也看不到其他 PID Namespace 里的具体情况\n障眼法 # Docker 项目帮助用户启动的，还是原来的应用进程，只不过在创建这些进程时， Docker 为它们加上了各种各样的 Namespace 参数。\n这时，这些进程就会觉得自己是各自 PID Namespace 里的第 1 号进程， 只能看到各自 Mount Namespace 里挂载的目录和文件， 只能访问到各自 Network Namespace 里的网络设备， 就仿佛运行在一个个“容器”里面，与世隔绝。\nNamespace 技术实际上修改了应用进程看待整个计算机“视图”，即它的“视线”被操作系统做了限制，只能“看到”某些指定的内容\n\u0026ldquo;敏捷\u0026quot;和\u0026quot;高性能\u0026quot;是容器相较于虚拟机最大的优势.\n隔离得不彻底 # 容器只是运行在宿主机上的一种特殊的进程，那么多个容器之间使用的就还是同一个宿主机的操作系统内核\n行不通\nWindows 宿主机上运行 Linux 容器 或者在低版本的 Linux 宿主机上运行高版本的 Linux 容器 在 Linux 内核中，有很多资源和对象是不能被Namespace化的\n比如时间.如果你的容器中的程序使用 settimeofday(2) 系统调用修改了时间，整个宿主机的时间都会被随之修改\nLinux Cgroups # Linux 内核中用来为进程设置资源限制的一个重要功能\nLinux Cgroups 的全称是 Linux Control Group。 它最主要的作用，就是限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等.\nLinux Cgroups 的设计还是比较易用的，简单粗暴地理解呢，它就是一个子系统目录加上一组资源限制文件的组合.\nLinux Cgroups不足:/proc 文件系统.\nMount # Mount Namespace 修改的，是容器进程对文件系统“挂载点”的认知.\n它对容器进程视图的改变，一定是伴随着挂载操作（mount）才能生效.\nMount Namespace 正是基于对 chroot 的不断改良才被发明出来的，它也是 Linux 操作系统里的第一个 Namespace.\n而这个挂载在容器根目录上、用来为容器进程提供隔离后执行环境的文件系统，就是所谓的“容器镜像”。它还有一个更为专业的名字，叫作：rootfs（根文件系统）\n容器诞生\n启用 Linux Namespace 配置； 设置指定的 Cgroups 参数； 切换进程的根目录（Change Root）// 优先使用 pivot_root 系统调用. rootfs 只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。 在 Linux 操作系统中，这两部分是分开存放的，操作系统只有在开机启动时才会加载指定版本的内核镜像.\n正是由于 rootfs 的存在，容器才有了一个被反复宣传至今的重要特性：一致性\n由于 rootfs 里打包的不只是应用，而是整个操作系统的文件和目录，也就意味着， 应用以及它运行所需要的所有依赖，都被封装在了一起.\n对一个应用来说，操作系统本身才是它运行所需要的最完整的“依赖库” 难道我每开发一个应用，或者升级一下现有的应用，都要重复制作一次 rootfs 吗？ Docker 在镜像的设计中，引入了层（layer）的概念。也就是说，用户制作镜像的每一步操作，都会生成一个层，也就是一个增量 rootfs。\n联合文件系统 # 联合文件系统（Union File System）,也叫 UnionFS. 最主要的功能是将多个不同位置的目录联合挂载（union mount）到同一个目录下.\nAuFS # AuFS 的全称是 Another UnionFS，后改名为 Alternative UnionFS，再后来干脆改名叫作 Advance UnionFS， 从这些名字中你应该能看出这样两个事实：\n它是对 Linux 原生 UnionFS 的重写和改进； 它的作者怨气好像很大。我猜是 Linus Torvalds（Linux 之父）一直不让 AuFS 进入 Linux 内核主干的缘故，所以我们只能在 Ubuntu 和 Debian 这些发行版上使用它。 OverlayFS better than AuFS,AuFS 已经成为历史.\n分层\n只读层(ro+wh,readonly+whiteout) Init 层 // 存放 /etc/hosts、/etc/resolv.conf 等信息 可读写层 // 一旦在容器里做了写操作，你修改产生的内容就会以增量的方式出现在这个层中. whiteout\n如果我现在要做的，是删除只读层里的一个文件呢？\n为了实现这样的删除操作，AuFS 会在可读写层创建一个 whiteout 文件，把只读层里的文件“遮挡”起来\n应用容器化 # 制作容器镜像 # Dockerfile 的设计思想，是使用一些标准的原语（即大写高亮的词语），描述我们所要构建的 Docker 镜像。 并且这些原语，都是按顺序处理的.\nCMD 都是 Docker 容器进程启动所必需的参数，完整执行格式是：“ENTRYPOINT CMD”. 默认情况下，Docker 会为你提供一个隐含的 ENTRYPOINT，即：/bin/sh -c.\nDockerfile 中的每个原语执行后，都会生成一个对应的镜像层\ndocker exec 的实现原理\n一个进程，可以选择加入到某个进程已有的 Namespace 当中，从而达到“进入”这个进程所在容器的目的\nNetwork # Docker 还专门提供了一个参数，可以让你启动一个容器并“加入”到另一个容器的 Network Namespace 里，这个参数就是 -net\n–net=host，就意味着这个容器不会为进程启用 Network Namespace。这就意味着， 这个容器拆除了 Network Namespace 的“隔离墙”，所以，它会和宿主机上的其他普通进程一样，直接共享宿主机的网络栈。 这就为容器直接操作和使用宿主机网络提供了一个渠道。\nVolume # Volume 机制解决问题\n容器里进程新建的文件，怎么才能让宿主机获取到？ 宿主机上的文件和目录，怎么才能让容器里的进程访问到？ 允许你将宿主机上指定的目录或者文件，挂载到容器里面进行读取和修改操作\n执行挂载操作时，“容器进程”已经创建了，也就意味着此时 Mount Namespace 已经开启了。所以，这个挂载事件只在这个容器里可见。你在宿主机上，是看不见容器内部的这个挂载点的。这就保证了容器的隔离性不会被 Volume 打破\n注意：这里提到的 \u0026quot; 容器进程 \u0026ldquo;，是 Docker 创建的一个容器初始化进程 (dockerinit)，而不是应用进程 (ENTRYPOINT + CMD)。dockerinit 会负责完成根目录的准备、挂载设备和目录、配置 hostname 等一系列需要在容器内进行的初始化操作。最后，它通过 execv() 系统调用，让应用进程取代自己，成为容器里的 PID=1 的进程。\nLinux 的绑定挂载（bind mount）机制\n它的主要作用就是，允许你将一个目录或者文件，而不是整个设备，挂载到一个指定的目录上。并且，这时你在该挂载点上进行的任何操作，只是发生在被挂载的目录或者文件上，而原挂载点的内容则会被隐藏起来且不受影响\n在一个正确的时机，进行一次绑定挂载，Docker 就可以成功地将一个宿主机上的目录或文件，不动声色地挂载到容器中.\ndocker run -d -v /test helloworld\n容器 Volume 里的信息，并不会被 docker commit 提交掉；但这个挂载点目录 /test 本身，则会出现在新的镜像当中.\nKubernetes的本质 # 一个正在运行的 Linux 容器\n一组联合挂载在 /var/lib/docker/aufs/mnt 上的 rootfs，这一部分我们称为“容器镜像”（Container Image），是容器的静态视图； 一个由 Namespace+Cgroups 构成的隔离环境，这一部分我们称为“容器运行时”（Container Runtime），是容器的动态视图。 容器就从一个开发者手里的小工具，一跃成为了云计算领域的绝对主角；而能够定义容器组织和管理规范的“容器编排”技术，则当仁不让地坐上了容器技术领域的“头把交椅”.\nk8s架构图 节点\nMaster 控制节点 API 服务 kube-apiserver 负责调度 kube-scheduler 负责容器编排 kube-controller-manager 整个集群的持久化数据，则由 kube-apiserver 处理后保存在 Etcd 中 Node 计算节点 kubelet 负责同容器运行时（比如 Docker 项目）打交道 kubelet # kubelet交互所依赖的，是一个称作 CRI（Container Runtime Interface）的远程调用接口， 这个接口定义了容器运行时的各项核心操作，比如：启动一个容器需要的所有参数.\n通过 gRPC 协议同一个叫作 Device Plugin 的插件进行交互 接口为CNI（Container Networking Interface） 调用网络插件和存储插件为容器配置网络和持久化存储 接口为CSI（Container Storage Interface） 观点:\n运行在大规模集群中的各种任务之间，实际上存在着各种各样的关系。这些关系的处理，才是作业编排和管理系统最困难的地方。\nKubernetes 项目最主要的设计思想是，从更宏观的角度，以统一的方式来定义任务之间的各种关系，并且为将来支持更多种类的关系留有余地\nService 服务的主要作用，就是作为 Pod 的代理入口（Portal），从而代替 Pod 对外暴露一个固定的网络地址\n除了应用与应用之间的关系外，应用运行的形态是影响“如何容器化这个应用”的第二个重要因素。\n在 Kubernetes 项目中，我们所推崇的使用方法是：\n首先，通过一个“编排对象”，比如 Pod、Job、CronJob 等，来描述你试图管理的应用； 然后，再为它定义一些“服务对象”，比如 Service、Secret、Horizontal Pod Autoscaler（自动水平扩展器）等。这些对象，会负责具体的平台级功能。 这种使用方法，就是所谓的“声明式 API”。这种 API 对应的“编排对象”和“服务对象”，都是 Kubernetes 项目中的 API 对象（API Object）。\nkubeadm # 让用户能够通过这样两条指令完成一个 Kubernetes 集群的部署\n# 创建一个 Master 节点 $ kubeadm init # 将一个 Node 节点加入到当前集群中 $ kubeadm join \u0026lt;Master 节点的 IP 和端口 \u0026gt; 把 kubelet 直接运行在宿主机上，然后使用容器部署其他的 Kubernetes 组件。\n"},{"id":17,"href":"/coding/docs/ms/k8s_network/","title":"k8s网络","section":"微服务","content":" 网络栈 # 网卡（Network Interface） 回环设备（Loopback Device） 路由表（Routing Table） iptables 规则 被隔离在它自己的 Network Namespace 当中的\n在大多数情况下，我们都希望容器进程能使用自己 Network Namespace 里的网络栈，即：拥有属于自己的 IP 地址和端口\n"},{"id":18,"href":"/coding/docs/ms/kratos/","title":"kratos框架","section":"微服务","content":" 服务发现 # 熔断 # 用于提供客户端熔断功能\n限流 # 用于服务端流量控制\n其他 # 重试 降级 排队 "},{"id":19,"href":"/coding/docs/go/map/","title":"map","section":"golang","content":" 结构 # // Go map 的头部。 type hmap struct { // 注意：hmap 的格式也被编码在 cmd/compile/internal/reflectdata/reflect.go 中。 // 确保这个定义与编译器的定义保持同步。 count int // 存储在 map 中的键值对数量。必须是第一个字段（用于内置函数 len()） flags uint8 // 表示 map 的状态标志，包括了迭代器是否在使用中、是否正在进行扩容等信息。 B uint8 // 存储桶的数量的对数，实际桶的数量为len(buckets) == 2^B(bucketShift bucket的位移值)。 noverflow uint16 // 溢出桶数量的估计值。 number of overflows hash0 uint32 // 哈希种子。 buckets unsafe.Pointer // 存储键值对的桶数组，其长度为 2^B。如果 count 为 0，则可能为 nil。 oldbuckets unsafe.Pointer // 扩容时旧的桶数组，长度为 2^(B-1)，用于数据搬迁。如果没有扩容，则为 nil。 nevacuate uintptr // 扩容时已经完成搬迁的桶数量。 \u0026#34;not evacuate\u0026#34; extra *mapextra // 可选字段，指向了一些额外的 map 属性，例如 map 的类型信息和哈希函数。 } // mapextra 包含了一些不是所有 map 都有的字段。 type mapextra struct { // 如果 key 和 elem 都不包含指针，并且它们都可以内联，那么我们标记 bucket 的类型不包含指针。 // 这样可以避免扫描这样的 map。 // 然而，bmap.overflow 是一个指针。为了保持溢出桶的存活状态，我们在 hmap.extra.overflow 和 hmap.extra.oldoverflow 中存储了指向所有溢出桶的指针。 // 只有当 key 和 elem 都不包含指针时才使用 overflow 和 oldoverflow。 // overflow 存储 hmap.buckets 的溢出桶。 // oldoverflow 存储 hmap.oldbuckets 的溢出桶。 // 间接存储允许在 hiter 中存储一个指向切片的指针。 overflow *[]*bmap oldoverflow *[]*bmap // nextOverflow 指向一个空闲的溢出桶。 nextOverflow *bmap } // Go map 的 bucket。 bmap equal bucket map type bmap struct { // tophash 通常包含此 bucket 中每个 key 的 hash 值的高字节。 存储了键的哈希的高 8 位 // 如果 tophash[0] \u0026lt; minTopHash，则 tophash[0] 是一个 bucket 撤离状态。 tophash [bucketCnt]uint8 // bucketCnt equal bucket count // 接下来是 bucketCnt 个 key，然后是 bucketCnt 个 elem。 // 注意：将所有 key 放在一起，然后将所有 elem 放在一起比交替 key/elem/key/elem/... 代码更复杂，但可以消除需要填充的情况，例如 map[int64]int8。 // 最后是一个溢出指针。 } src/cmd/compile/internal/reflectdata/reflect.go::MapBucketType type bmap struct { topbits [8]uint8 keys [8]keytype values [8]valuetype pad uintptr overflow uintptr } "},{"id":20,"href":"/coding/docs/nosql/mongodb/","title":"mongoDB","section":"nosql","content":"https://www.mongodb.com/docs/\n客户端推荐 compass or navicat\n服务端由cpp编写.\nmongo 客户端,类似redis-cli mongod 服务端启动,类似mysqld 配置\n日志目录 数据存储目录 守护进程方式启动 绑定ip启动 端口号 27017 命令\n#显示数据库 show dbs #切换数据库 use \u0026lt;数据库\u0026gt; #打印当前数据库 db #删库操作 db.dropDatabase() # 创建集合 db.createCollection(name) # 显示集合 show collections # 删除集合 db.\u0026lt;集合\u0026gt;.drop() 数据库\n#后台权限库 admin curd # # 集合变量的插入 https://www.mongodb.com/docs/manual/tutorial/insert-documents/ db.collection.insert() # 集合变量的多个插入 db.collection.insertMany() 查找\n# 集合查找 db.collection.find() # 集合带条件查找 db.collection.find({}) # 集合查找1条 db.collection.findOne() # 让user字段显示,让name字段不显示出来 db.collection.find({},{user:1,name:0}) # 捕获错误 try{}catch(e){ print(e) } 更新\n# 覆盖更新 db.collection.update() #局部修改 db.collection.update({},{$set:{}}) # 更新多条 db.collection.update({},{},{multi:true}) # 自增操作 db.collection.update({},{$inc:{}}) 删除\n# 删除 db.collection.remove() 高级查询 # db.collection.count() # 统计查询 db.collection.find().limit(2) # 页数 db.collection.find().limit(2).skip(4) # 页数 跳过多少条 db.collection.find().sort({key:1}) # 升序 db.collection.find().sort({key:-1}) # 降序 正则查询\ndb.collection.find(key:/expr/) # 正则匹配查询 比较查询\ndb.collection.find({key:{$gt:100 }}) # 大于操作 db.collection.find({key:{$in: [\u0026#34;1001\u0026#34;,\u0026#34;1002\u0026#34;] }}) # where in 条件查询\n$and:[{},{},{}] $or:[{},{},{}] db.collection.find({$or:[{user:\u0026ldquo;1\u0026rdquo;},{\u0026ldquo;name\u0026rdquo;:\u0026ldquo;2\u0026rdquo;}]}) 索引 # 单个索引 联合索引 地理索引 文本索引 哈希索引 TTL索引 查看索引\ndb.collection.getIndexes() # 查询所有索引 创建索引\ndb.collection.createIndex(keys,option) # 创建索引 db.collection.createIndex({userid:1}) # 升序索引 db.collection.createIndex({userid:1,name:-1}) # 联合索引 删除索引\ndb.collection.dropIndex(index) # 删除索引 索引名词来删 db.collection.dropIndexes() #删所有索引 性能排查 # db.collection().find().explain() # 副本集 # 类似redis哨兵模式\n分片集群 # 路由-\u0026gt;配置服务-\u0026gt;分片\n安全认证 # 权限\nread readWrite root #超管 userAdmin # 数据库创建和修改用户 副本集安全认证\n通过key文件 "},{"id":21,"href":"/coding/docs/web/openresty/","title":"OpenResty","section":"web","content":" 简介 # OpenResty 是一个兼具开发效率和性能的服务端开发平台，基于 NGINX 实现，包含反向代理和负载均衡.\n核心 # NGINX 的一个 C 模块（lua-nginx-module）.\n该模块将 LuaJIT 嵌入到 NGINX 服务器中，并对外提供一套完整的 Lua API，透明地支持非阻塞 I/O，提供了轻量级线程、定时器等高级抽象。\n同时，围绕这个模块，OpenResty 构建了一套完备的测试框架、调试技术以及由 Lua 实现的周边功能库\n用Lua 语言来进行 字符串和数值运算 查询数据库 发送 HTTP 请求 执行定时任务 调用外部命令 \u0026hellip; 可以用 FFI 的方式调用外部 C 函数 特性 # OpenResty作者章亦春, 就职于淘宝,后就职于Cloudflare\n详尽的文档和测试用例 同步非阻塞 动态 例子 # cli # $ resty -e \u0026#34;ngx.say(\u0026#39;hello world\u0026#39;)\u0026#34; hello world web # events { worker_connections 1024; } http { server { listen 8080; location / { content_by_lua \u0026#39; ngx.say(\u0026#34;hello, world\u0026#34;) \u0026#39;; } } } openresty -p `pwd` -c conf/nginx.conf $ curl -i 127.0.0.1:8080 HTTP/1.1 200 OK Server: openresty/1.13.6.2 Content-Type: text/plain Transfer-Encoding: chunked Connection: keep-alive hello, world lua文件 # $ mkdir lua $ cat lua/hello.lua ngx.say(\u0026#34;hello, world\u0026#34;) pid logs/nginx.pid; events { worker_connections 1024; } http { server { listen 8080; location / { content_by_lua_file lua/hello.lua; } } } 重启 OpenResty 的服务\n$ sudo kill -HUP `cat logs/nginx.pid` nginx # OpenResty 的作者多年前写过一个nginx教程\nNGINX 有 11 个执行阶段\ntypedef enum { NGX_HTTP_POST_READ_PHASE = 0, NGX_HTTP_SERVER_REWRITE_PHASE, NGX_HTTP_FIND_CONFIG_PHASE, NGX_HTTP_REWRITE_PHASE, NGX_HTTP_POST_REWRITE_PHASE, NGX_HTTP_PREACCESS_PHASE, NGX_HTTP_ACCESS_PHASE, NGX_HTTP_POST_ACCESS_PHASE, NGX_HTTP_PRECONTENT_PHASE, NGX_HTTP_CONTENT_PHASE, NGX_HTTP_LOG_PHASE } ngx_http_phases; OpenResty 也有 11 个 *_by_lua指令 init_by_lua 只会在 Master 进程被创建时执行， init_worker_by_lua 只会在每个 Worker 进程被创建时执行。 其他的 *_by_lua 指令则是由终端请求触发，会被反复执行 业务代码\nset_by_lua：设置变量； rewrite_by_lua：转发、重定向等； access_by_lua：准入、权限等； content_by_lua：生成返回内容； header_filter_by_lua：应答头过滤处理； body_filter_by_lua：应答体过滤处理； log_by_lua：日志记录。 其他能用 Lua 代码解决的，尽量用代码来解决，而非使用Nginx 的模块和配置\nluajit # $ cat 1.lua print(\u0026#34;hello world\u0026#34;) $ luajit 1.lua hello world "},{"id":22,"href":"/coding/docs/mq/rabbitmq/","title":"rabbitmq","section":"mq","content":" 延迟队列实现 # TTL+死信队列，组合实现延迟队列的效果\nTTL 消息过期时间设置 # TTL，全称Time To Live，消息过期时间设置。消息的TTL就是消息的存活时间。RabbitMQ可以对队列和消息分别设置TTL。对队列设置就是队列没有消费者连着的保留时间，也可以对每一个单独的消息做单独的设置。超过了这个时间，我们认为这个消息就死了，称之为死信。 队列过期后，会将队列所有消息全部移除。 一个队列中某一个消息过期后，只有消息在队列顶端，才会判断其是否过期(移除掉)，如果不在队列顶端，那么是无效的，过期时间有队列的过期时间判定。 如果队列设置了，消息也设置了，那么会取时间短的。所以一个消息如果被路由到不同的队列中，这个消息死亡的时间有可能不一样（不同的队列设置）。 我门一般通过设置消息的x-message-ttl属性来设置时间\n死信队列 # 死信队列，英文缩写：DLX 。Dead Letter Exchange（死信交换机），当消息成为Dead message后，可以被重新发送到另一个交换机，这个交换机就是DLX。\n消息成为死信的三种情况：\n队列消息长度到达限制 消费者拒接消费消息，basicNack/basicReject,并且不把消息重新放入原目标队 列,requeue=false 原队列存在消息过期设置，消息到达超时时间未被消费 队列绑定死信交换机，给队列设置参数： x-dead-letter-exchange 和 x-dead-letter-routing-key，就能成功绑定了\nDead Letter Exchange其实就是一种普通的exchange，和创建其他exchange没有两样。只是在某一个设置Dead Letter Exchange的队列中有消息过期了，会自动触发消息的转发，发送到Dead Letter Exchange中去。\n实现 # 延迟任务通过消息的TTL和Dead Letter Exchange来实现。 我们需要建立2个队列，一个用于发送消息，一个用于消息过期后的转发目标队列。\n场景\n订单超时取消 应用 # 应用解耦 流量削锋 异步消息 "},{"id":23,"href":"/coding/docs/distributed/raft-etcd/","title":"raft与etcd","section":"分布式","content":" 主要解决问题点 # Leader选举，Leader故障后集群能快速选出新Leader； 日志复制， 集群只有Leader能写入日志， Leader负责复制日志到Follower节点，并强制Follower节点与自己保持相同； 安全性，一个任期内集群只能产生一个Leader、已提交的日志条目在发生Leader选举时，一定会存在更高任期的新Leader日志中、各个节点的状态机应用的任意位置的日志条目内容应一样等。 角色 # 角色 说明 Leader 主节点 同一时刻只有一个 Leader，负责协调和管理其他节点. 唯一性，拥有同步日志的特权，需定时广播心跳给Follower节点，以维持领导者身份。 Candidate 候选者 每一个节点都可以成为 Candidate，节点在该角色下才可以被选为新的 Leader Follower Leader 的跟随者 不可以发起选举 当Follower节点接收Leader节点心跳消息超时后，它会转变成Candidate节点，并可发起竞选Leader投票，若获得集群多数节点的支持后，它就可转变成Leader节点\ngraph RL F[Follower] C[Candidate] L[Leader] L--\u003e|发现更大term|F C--\u003e|发现有了新的任期或新主|F F --\u003e|长时间没有收到leader消息,开始竞选|C C--\u003e|收到一半以上选票|L 流程 # 初始化时，所有节点均为 Follower 状态。 开始选主时，所有节点的状态由 Follower 转化为 Candidate，并向其他节点发送选举请求。 其他节点根据接收到的选举请求的先后顺序，回复是否同意成为主。这里需要注意的是，在每一轮选举中，一个节点只能投出一张票。 若发起选举请求的节点获得超过一半的投票，则成为主节点，其状态转化为 Leader，其他节点的状态则由 Candidate 降为 Follower。Leader 节点与 Follower 节点之间会定期发送心跳包，以检测主节点是否活着。 当 Leader 节点的任期到了，即发现其他服务器开始下一轮选主周期时，Leader 节点的状态由 Leader 降级为 Follower，进入新一轮选主。 etcd Leader选举原理 # 下面以Leader crash场景为案例，介绍一下etcd Leader选举原理\n假设集群总共3个节点，A节点为Leader，B、C节点为Follower。\nLeader维持身份 # 如上Leader选举图左边部分所示， 正常情况下，Leader节点会按照心跳间隔时间，定时广播心跳消息（MsgHeartbeat消息）给Follower节点，以维持Leader身份。 Follower收到后回复心跳应答包消息（MsgHeartbeatResp消息）给Leader。\n细心的你可能注意到上图中的Leader节点下方有一个任期号（term）， 它具有什么样的作用呢？\n这是因为Raft将时间划分成一个个任期，任期用连续的整数表示，每个任期从一次选举开始，赢得选举的节点在该任期内充当Leader的职责，随着时间的消逝，集群可能会发生新的选举，任期号也会单调递增。\n通过任期号，可以比较各个节点的数据新旧、识别过期的Leader等，它在Raft算法中充当逻辑时钟，发挥着重要作用。\n自愈 # Leader crash后，etcd是如何自愈的呢?\n如上Leader选举图右边部分所示，当Leader节点异常后，Follower节点会接收Leader的心跳消息超时，当超时时间大于竞选超时时间后，它们会进入Candidate状态。\n这里要提醒下你，etcd默认心跳间隔时间（heartbeat-interval）是100ms， 默认竞选超时时间（election timeout）是1000ms， 你需要根据实际部署环境、业务场景适当调优，否则就很可能会频繁发生Leader选举切换，导致服务稳定性下降。\n进入Candidate状态的节点，会立即发起选举流程，自增任期号，投票给自己，并向其他节点发送竞选Leader投票消息（MsgVote）。\nC节点收到Follower B节点竞选Leader消息后，这时候可能会出现如下两种情况：\nC节点判断B节点的数据至少和自己一样新、B节点任期号大于C当前任期号、并且C未投票给其他候选者，就可投票给B。这时B节点获得了集群多数节点支持，于是成为了新的Leader。\n恰好C也心跳超时超过竞选时间了，它也发起了选举，并投票给了自己，那么它将拒绝投票给B，这时谁也无法获取集群多数派支持，只能等待竞选超时，开启新一轮选举。\nRaft为了优化选票被瓜分导致选举失败的问题，引入了随机数，每个节点等待发起选举的时间点不一致，优雅的解决了潜在的竞选活锁，同时易于理解。 Leader选出来后，它什么时候又会变成Follower状态呢？ 从上面的状态转换关系图中你可以看到，如果现有Leader发现了新的Leader任期号，那么它就需要转换到Follower节点。A节点crash后，再次启动成为Follower，假设因为网络问题无法连通B、C节点，这时候根据状态图， 我们知道它将不停自增任期号，发起选举。等A节点网络异常恢复后，那么现有Leader收到了新的任期号，就会触发新一轮Leader选举，影响服务的可用性。\n然而A节点的数据是远远落后B、C的，是无法获得集群Leader地位的，发起的选举无效且对集群稳定性有伤害。\n那如何避免以上场景中的无效的选举呢？\n在etcd 3.4中，etcd引入了一个PreVote参数（默认false），可以用来启用PreCandidate状态解决此问题，如下图所示。 Follower在转换成Candidate状态前，先进入PreCandidate状态，不自增任期号， 发起预投票。若获得集群多数节点认可，确定有概率成为Leader才能进入Candidate状态，发起选举流程。 因A节点数据落后较多，预投票请求无法获得多数节点认可，因此它就不会进入Candidate状态，导致集群重新选举。\n这就是Raft Leader选举核心原理，使用心跳机制维持Leader身份、触发Leader选举，etcd基于它实现了高可用，只要集群一半以上节点存活、可相互通信，Leader宕机后，就能快速选举出新的Leader，继续对外提供服务。\n日志复制 # Raft日志复制原理 # 假设在上面的Leader选举流程中，B成为了新的Leader，它收到put提案后，它是如何将日志同步给Follower节点的呢？ 什么时候它可以确定一个日志条目为已提交，通知etcdserver模块应用日志条目指令到状态机呢？\nLeader收到put请求后，向Follower节点复制日志的整体流程图\n首先Leader收到client的请求后，etcdserver的KV模块会向Raft模块提交一个put hello为world提案消息（流程图中的序号2流程）， 它的消息类型是MsgProp。\nLeader的Raft模块获取到MsgProp提案消息后，为此提案生成一个日志条目，追加到未持久化、不稳定的Raft日志中，随后会遍历集群Follower列表和进度信息，为每个Follower生成追加（MsgApp）类型的RPC消息，此消息中包含待复制给Follower的日志条目。\nRaft日志 # 下图是Raft日志复制过程中的日志细节图，简称日志图1。\n在日志图中，最上方的是日志条目序号/索引，日志由有序号标识的一个个条目组成，每个日志条目内容保存了Leader任期号和提案内容。最开始的时候，A节点是Leader，任期号为1，A节点crash后，B节点通过选举成为新的Leader， 任期号为2。\n日志图1描述的是hello日志条目未提交前的各节点Raft日志状态。 Leader是如何知道从哪个索引位置发送日志条目给Follower，以及Follower已复制的日志最大索引是多少呢？ Leader会维护两个核心字段来追踪各个Follower的进度信息，\n一个字段是NextIndex， 它表示Leader发送给Follower节点的下一个日志条目索引。\n一个字段是MatchIndex， 它表示Follower节点已复制的最大日志条目的索引，比如上面的日志图1中C节点的已复制最大日志条目索引为5，A节点为4。\n日志条目什么时候才会追加到稳定的Raft日志中呢？Raft模块负责持久化吗？ etcd Raft模块设计实现上抽象了网络、存储、日志等模块，它本身并不会进行网络、存储相关的操作，上层应用需结合自己业务场景选择内置的模块或自定义实现网络、存储、日志等模块。\n上层应用通过Raft模块的输出接口（如Ready结构），获取到待持久化的日志条目和待发送给Peer节点的消息后（如上面的MsgApp日志消息），需持久化日志条目到自定义的WAL模块，通过自定义的网络模块将消息发送给Peer节点。\n日志条目持久化到稳定存储中后，这时候你就可以将日志条目追加到稳定的Raft日志中。即便这个日志是内存存储，节点重启时也不会丢失任何日志条目，因为WAL模块已持久化此日志条目，可通过它重建Raft日志。\netcd Raft模块提供了一个内置的内存存储（MemoryStorage）模块实现，etcd使用的就是它，Raft日志条目保存在内存中。网络模块并未提供内置的实现，etcd基于HTTP协议实现了peer节点间的网络通信，并根据消息类型，支持选择pipeline、stream等模式发送，显著提高了网络吞吐量、降低了延时。\n分析etcd是如何与Raft模块交互，获取待持久化的日志条目和发送给peer节点的消息。 正如刚刚讲到的，Raft模块输入是Msg消息，输出是一个Ready结构，它包含待持久化的日志条目、发送给peer节点的消息、已提交的日志条目内容、线性查询结果等Raft输出核心信息。\netcdserver模块通过channel从Raft模块获取到Ready结构后（流程图中的序号3流程），因B节点是Leader，它首先会通过基于HTTP协议的网络模块将追加日志条目消息（MsgApp）广播给Follower，并同时将待持久化的日志条目持久化到WAL文件中（流程图中的序号4流程），最后将日志条目追加到稳定的Raft日志存储中（流程图中的序号5流程）。\n各个Follower收到追加日志条目（MsgApp）消息，并通过安全检查后，它会持久化消息到WAL日志中，并将消息追加到Raft日志存储，随后会向Leader回复一个应答追加日志条目（MsgAppResp）的消息，告知Leader当前已复制的日志最大索引（流程图中的序号6流程）。\nLeader收到应答追加日志条目（MsgAppResp）消息后，会将Follower回复的已复制日志最大索引更新到跟踪Follower进展的Match Index字段，如下面的日志图2中的Follower C MatchIndex为6，Follower A为5，日志图2描述的是hello日志条目提交后的各节点Raft日志状态。 最后Leader根据Follower的MatchIndex信息，计算出一个位置，如果这个位置已经被一半以上节点持久化，那么这个位置之前的日志条目都可以被标记为已提交。\n在我们这个案例中日志图2里6号索引位置之前的日志条目已被多数节点复制，那么他们状态都可被设置为已提交。Leader可通过在发送心跳消息（MsgHeartbeat）给Follower节点时，告知它已经提交的日志索引位置。\n最后各个节点的etcdserver模块，可通过channel从Raft模块获取到已提交的日志条目（流程图中的序号7流程），应用日志条目内容到存储状态机（流程图中的序号8流程），返回结果给client。\n通过以上流程，Leader就完成了同步日志条目给Follower的任务，一个日志条目被确定为已提交的前提是，它需要被Leader同步到一半以上节点上。以上就是etcd Raft日志复制的核心原理。\n安全性 # 如果在上面的日志图2中，Leader B在应用日志指令put hello为world到状态机，并返回给client成功后，突然crash了，那么Follower A和C是否都有资格选举成为Leader呢？\n从日志图2中我们可以看到，如果A成为了Leader那么就会导致数据丢失，因为它并未含有刚刚client已经写入成功的put hello为world指令。\nRaft算法如何确保面对这类问题时不丢数据和各节点数据一致性呢？\n这就是Raft的第三个子问题需要解决的。Raft通过给选举和日志复制增加一系列规则，来实现Raft算法的安全性。\n选举规则 # 当节点收到选举投票的时候，需检查候选者的最后一条日志中的任期号，若小于自己则拒绝投票。如果任期号相同，日志却比自己短，也拒绝为其投票。\n比如在日志图2中，Folllower A和C任期号相同，但是Follower C的数据比Follower A要长，那么在选举的时候，Follower C将拒绝投票给A， 因为它的数据不是最新的。\n同时，对于一个给定的任期号，最多只会有一个leader被选举出来，leader的诞生需获得集群一半以上的节点支持。每个节点在同一个任期内只能为一个节点投票，节点需要将投票信息持久化，防止异常重启后再投票给其他节点。\n通过以上规则就可防止日志图2中的Follower A节点成为Leader。\n日志复制规则 # 在日志图2中，Leader B返回给client成功后若突然crash了，此时可能还并未将6号日志条目已提交的消息通知到Follower A和C，那么如何确保6号日志条目不被新Leader删除呢？ 同时在etcd集群运行过程中，Leader节点若频繁发生crash后，可能会导致Follower节点与Leader节点日志条目冲突，如何保证各个节点的同Raft日志位置含有同样的日志条目？\n以上各类异常场景的安全性是通过Raft算法中的Leader完全特性和只附加原则、日志匹配等安全机制来保证的。\nLeader完全特性是指如果某个日志条目在某个任期号中已经被提交，那么这个条目必然出现在更大任期号的所有Leader中。\nLeader只能追加日志条目，不能删除已持久化的日志条目（只附加原则），因此Follower C成为新Leader后，会将前任的6号日志条目复制到A节点。\n为了保证各个节点日志一致性，Raft算法在追加日志的时候，引入了一致性检查。Leader在发送追加日志RPC消息时，会把新的日志条目紧接着之前的条目的索引位置和任期号包含在里面。Follower节点会检查相同索引位置的任期号是否与Leader一致，一致才能追加，这就是日志匹配特性。它本质上是一种归纳法，一开始日志空满足匹配特性，随后每增加一个日志条目时，都要求上一个日志条目信息与Leader一致，那么最终整个日志集肯定是一致的。\n通过以上的Leader选举限制、Leader完全特性、只附加原则、日志匹配等安全特性，Raft就实现了一个可严格通过数学反证法、归纳法证明的高可用、一致性算法，为etcd的安全性保驾护航。 哪些场景会出现 Follower 日志与 Leader 冲突？ leader崩溃的情况下可能(如老的leader可能还没有完全复制所有的日志条目)，如果leader和follower出现持续崩溃会加剧这个现象。follower可能会丢失一些在新的leader中有的日志条目，他也可能拥有一些leader没有的日志条目，或者两者都发生。 etcd WAL模块只能持续追加日志条目,follower如何删除无效日志？ leader处理不一致是通过强制follower直接复制自己的日志来解决。因此在follower中的冲突的日志条目会被leader的日志覆盖。leader会记录follower的日志复制进度nextIndex，如果follower在追加日志时一致性检查失败，就会拒绝请求，此时leader就会减小 nextIndex 值并进行重试，最终在某个位置让follower跟leader一致。\n为什么WAL日志模块只通过追加，也能删除已持久化冲突的日志条目呢？\n其实这里etcd在实现上采用了一些比较有技巧的方法，在WAL日志中的确没删除废弃的日志条目，你可以在其中搜索到冲突的日志条目。只是etcd加载WAL日志时，发现一个raft log index位置上有多个日志条目的时候，会通过覆盖的方式，将最后写入的日志条目追加到raft log中，实现了删除冲突日志条目效果, 更多细节查看关于这个问题的讨论。\n优点 # 选举速度快 算法复杂度低 易于实现 缺点 # 它要求系统内每个节点都可以相互通信，且需要获得过半的投票数才能选主成功，因此通信量大。该算法选举稳定性比 Bully 算法好，这是因为当有新节点加入或节点故障恢复后，会触发选主，但不一定会真正切主，除非新节点或故障后恢复的节点获得投票数过半，才会导致切主\n"},{"id":24,"href":"/coding/docs/distributed/saas/","title":"SaaS多租户存储设计","section":"分布式","content":" 数据隔离模式 # 独立数据库 实例成本高 共享数据库但隔离Schema 租户之间共享工作负载,需要迁移出大租户. 统计计费困难 数据表字段区分 需要代码兼容,研发成本高,数据安全性低. 框架流程\n租户id解析,在handle业务请求时,解析出当前租户id 将租户id放入context中 根据租户id调用多租户存储连接器获取当前租户数据实例 代码层实现scope,在method行为前后进行注入. "},{"id":25,"href":"/coding/docs/go/select/","title":"select","section":"golang","content":" select作用 # case后面必须是io操作 监听case，没满足阻塞 有满足，任选1个执行 default 处理case都不满足情况 select不产生忙轮询 select 自身不带有循环机制 需要借助for break跳出一个选项 "},{"id":26,"href":"/coding/docs/go/slice/","title":"slice","section":"golang","content":" 数据结构 # type slice struct { array unsafe.Pointer // 指向底层数组的指针 len int // 切片的长度 cap int // 切片的容量 } 创建 # // 此时s的长度和容量都是5。 s := make([]int, 5) // 此时s的长度是3,容量是5。 s := make([]int, 3, 5) 取值 # // 该方式没有指定开始和结束索引,是从索引0开始取到结束,即取出slice a的全部元素。 a[:] // 指定开始索引startIndex,结束索引取到结尾。比如a[1:]会从索引1开始取到最后一个元素。 a[startIndex:] // 指定结束索引endIndex,开始索引从0开始。比如a[:3]会取索引0-2的三个元素。 a[:endIndex] // 指定开始索引和结束索引区间。比如a[1:3]会取索引1和2的两个元素。 a[startIndex:endIndex] // 三个索引除了开始和结束索引外,还可以指定容量capacity。这种方式可以对slice进行扩容或缩容操作。 a[startIndex:endIndex:capacity] a[:] - 取所有元素 a[1:] - 从索引1开始取到最后 a[:3] - 取索引0-2的三个元素 a[1:3] - 取索引1和2的两个元素 a[1:3:5] - 取索引1-2但是扩容为5 "},{"id":27,"href":"/coding/docs/redis/consistency/","title":"一致性风险","section":"redis","content":" 分布式锁的三个主要核心要素 # 安全性、互斥性。在同一时间内，不允许多个client同时获得锁。 活性。无论client出现crash还是遭遇网络分区，你都需要确保任意故障场景下，都不会出现死锁，常用的解决方案是超时和自动过期机制。 高可用、高性能。加锁、释放锁的过程性能开销要尽量低，同时要保证高可用，避免单点故障。 茅台超卖案例 # Redis——由分布式锁造成的重大事故\n仔细分析下来，可以发现，这个抢购接口在高并发场景下，是有严重的安全隐患的，主要集中在三个地方：\n没有其他系统风险容错处理 由于用户服务吃紧，网关响应延迟，但没有任何应对方式，这是超卖的导火索。\n看似安全的分布式锁其实一点都不安全 虽然采用了set key value [EX seconds] [PX milliseconds] [NX|XX]的方式，但是如果线程A执行的时间较长没有来得及释放，锁就过期了，此时线程B是可以获取到锁的。当线程A执行完成之后，释放锁，实际上就把线程B的锁释放掉了。这个时候，线程C又是可以获取到锁的，而此时如果线程B执行完释放锁实际上就是释放的线程C设置的锁。这是超卖的直接原因。\n非原子性的库存校验 非原子性的库存校验导致在并发场景下，库存校验的结果不准确。这是超卖的根本原因。\n通过以上分析，问题的根本原因在于库存校验严重依赖了分布式锁。\n因为在分布式锁正常set、del的情况下，库存校验是没有问题的。\n但是，当分布式锁不安全可靠的时候，库存校验就没有用了。\n其他风险 # 单Redis Master节点存在单点故障\n一主多备Redis实例又因为Redis主备异步复制，当Master节点发生crash时，可能会导致同时多个client持有分布式锁，违反了锁的安全性问题\n一般使用 setnx 方法，通过 Redis 实现锁和超时时间来控制锁的失效时间。但是在极端的情况下，当 Reids 主节点挂掉，但锁还没有同步到从节点时，根据哨兵机制，从就变成了主，继续提供服务。 这时，另外的线程可以再来请求锁，此时就会出现两个线程拿到了锁的情况。\nsetnx和expire命令分开写,没有原子性\nlua脚本 SET key value NX EX seconds 忘记设置过期时间\n存在app崩溃,导致锁永远无法释放 RedLock分布式锁 # 它基于多个独立的Redis Master节点工作，只要一半以上节点存活就能正常工作，同时不依赖Redis主备异步复制，具有良好的安全性、高可用性。 然而它的实现依赖于系统时间，当发生时钟跳变的时候，也会出现安全性问题\n"},{"id":28,"href":"/coding/docs/redis/lock/","title":"业务锁的选择","section":"redis","content":" etcd锁 vs redis锁 # 都可以用来保证多个服务或进程对共享资源的访问是互斥的.\n特性 etcd 锁 redis 锁 实现方式 基于 etcd 的 KV 数据库 基于 redis 的 set 数据结构 一致性 强一致性 弱一致性 性能 较低 较高 可靠性 较高 较低 分布式存储系统对比点\n数据存储 数据分布 数据复制 数据一致性 算法选型 盘点必要因素\n工作原理 优劣势 适用场景 技术实现 "},{"id":29,"href":"/coding/docs/mysql/transaction/","title":"事务","section":"mysql","content":" 事务特性 # 原子性 atomicity 一致性 consistency 隔离性 isolation 持久性 durability 如何保证事务特性?\n持久性是通过 redo log （重做日志）来保证的； 原子性是通过 undo log（回滚日志） 来保证的； 隔离性是通过 MVCC（多版本并发控制） 或锁机制来保证的； 一致性则是通过持久性+原子性+隔离性来保证； 并发问题 # 脏读 # 如果一个事务「读到」了另一个「未提交事务修改过的数据」，就意味着发生了「脏读」现象\n不可重复读 # 在一个事务内多次读取同一个数据，如果出现前后两次读到的数据不一样的情况，就意味着发生了「不可重复读」现象。\n幻读 # 在一个事务内多次查询某个符合查询条件的「记录数量」，如果出现前后两次查询到的记录数量不一样的情况，就意味着发生了「幻读」现象。\n事务隔离级别 # read uncommitted 读未提及 # 可能发生脏读、不可重复读和幻读现象\nread committed 读已提交 # 可能发生不可重复读和幻读现象\nrepeatable read 可重复读 # 可能发生幻读现象\nMySQL InnoDB 引擎的默认隔离级别\nserializable 串行化 # MySQL InnoDB 引擎的默认隔离级别虽然是「可重复读」，但是它很大程度上避免幻读现象\n针对快照读（普通 select 语句），是通过 MVCC 方式解决了幻读，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。 针对当前读（select \u0026hellip; for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读，因为当执行 select \u0026hellip; for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。 MVCC # multi-version concurrency control,多版本并发控制\n隐式字段 # 对于InnoDB存储引擎，每一行记录都有两个隐藏列DB_TRX_ID、DB_ROLL_PTR，如果表中没有主键和非NULL唯一键时，则还会有第三个隐藏的主键列DB_ROW_ID\nundo日志 # 快照读 # 读取的是记录数据的可见版本\n当前读 # 读取的是记录数据的最新版本\nRead View # m_ids ：指的是在创建 Read View 时，当前数据库中「活跃事务」的事务 id 列表，注意是一个列表，“活跃事务”指的就是，启动了但还没提交的事务。 min_trx_id ：指的是在创建 Read View 时，当前数据库中「活跃事务」中事务 id 最小的事务，也就是 m_ids 的最小值。 max_trx_id ：这个并不是 m_ids 的最大值，而是创建 Read View 时当前数据库中应该给下一个事务的 id 值，也就是全局事务中最大的事务 id 值 + 1； creator_trx_id ：指的是创建该 Read View 的事务的事务 id。 "},{"id":30,"href":"/coding/docs/mq/type/","title":"其他mq","section":"mq","content":" MQ Kafka RocketMQ RabbitMQ Redis实现 基于 Redis List 的 LPUSH 和 RPOP 的实现方式 基于 Redis 的订阅或发布模式 基于 Redis 的有序集合（Sorted Set）的实现方式 "},{"id":31,"href":"/coding/docs/go/data_type/","title":"数据类型","section":"golang","content":" 值类型 # int bool 引用类型 # chan slice map 指针 make 和 new # make 初始化内置的数据结构\nslice := make([]int, 0, 100) hash := make(map[int]bool, 10) ch := make(chan int, 5) new 返回类型指针\ni := new(int) var v int i := \u0026amp;v 支持平台 # // 可列出支持的平台 GOOS GOARCH go tool dist list "},{"id":32,"href":"/coding/docs/redis/data_type/","title":"数据类型","section":"redis","content":" 数据类型 # string hash list set sorted Set hyperLogLog Geo bitmap Stream 数据持久化 # RDB # Redis Database 会产生多个文件，每个文件代表某一个时刻的redis数据。对于aof来说，基于rdb恢复数据会更快。\nAOF # Append Only File 将每条写入命令写入日志中，在redis重启的时候通过日志文件重构数据\nredis故障Rdb会丢失更多的数据，Rdb快照都是隔5分钟或者更长时间生成，而aof每隔一秒就会执行一次，所以只会丢失一秒钟的数据。\n布隆过滤器 # 优点：优点很明显，二进制组成的数组，占用内存极少，并且插入和查询速度都足够快。\n缺点：随着数据的增加，误判率会增加；还有无法判断数据一定存在；另外还有一个重要缺点，无法删除数据\n场景 # 缓存击穿 # cache breakdown\n指一些很热的数据在缓存过期后,同时有大量请求集中打击数据库,一出现就“击穿”缓存去访问数据库\n热数据永不过期 加上互斥锁 缓存穿透 # cache penetration\n指查询一个本来不应该存在的数据,结果却没有穿透缓存查询后台,导致对数据库的空请求\n逻辑检查,小于1 或者是字符串不允许之类的 使用布隆过滤器 缓存空对象,如果是网络恶意攻击（每次key不一样，且数据库不存在），缓存占用了更多的内存,缓存空对象要考虑到缓存时间的设置 缓存雪崩 # cache avalanche\n过期时间设置随机值 分布式部署且均匀分布热点数据 热数据永不过期 服务降级 服务熔断 请求限流 redis高可用 # 主从复制 用于数据备份和读写分离 哨兵机制 自动故障转移提供高可用 集群机制 节点间数据同步和分区机制实现横向扩展和强一致性 redis过期策略 # 定时删除:在设置 key 的过期时间时，同时创建一个定时事件，当时间到达时，由事件处理器自动执行 key 的删除操作 定期删除:每隔一段时间删除 惰性删除:获取的时候判断是否过期 Redis 选择「惰性删除+定期删除」这两种策略配和使用\nredis的内存淘汰机制 # noeviction: 当内存不足时，新写入操作会报错 allkeys-lru：当内存不足时，移除最近最少使用的 key（这个是最常用的）。 allkeys-random：当内存不足时，随机移除某个 key。 volatile-lru：当内存不足时，在设置了过期时间的键中，移除最近最少使用的 key。 volatile-random：当内存不足时，在设置了过期时间的键中，随机移除某个 key。 volatile-ttl：当内存不足时，在设置了过期时间的键中，有更早过期时间的 key 优先移除。 "},{"id":33,"href":"/coding/docs/mysql/log/","title":"日志","section":"mysql","content":" undo log（回滚日志）：是 Innodb 存储引擎层生成的日志，实现了事务中的原子性，主要用于事务回滚和 MVCC。 redo log（重做日志）：是 Innodb 存储引擎层生成的日志，实现了事务中的持久性，主要用于掉电等故障恢复； binlog （归档日志）：是 Server 层生成的日志，主要用于数据备份和主从复制； relay log ,一般情况下它在MySQL主从同步读写分离集群的从节点才开启。主节点一般不需要这个日志。 "},{"id":34,"href":"/coding/docs/ms/discovery/","title":"服务发现","section":"微服务","content":" 服务发现 # star-history\nconsul etcd 配置中心 # 方案\napollo nacos "},{"id":35,"href":"/coding/docs/mysql/mysql_index/","title":"索引","section":"mysql","content":" 指定索引 # 使用FORCE INDEX关键字强制使用指定的索引,强制使用指定的索引\nSELECT * FROM table FORCE INDEX (index_name) WHERE condition; 使用USE INDEX关键字建议使用指定索引,优先使用指定的索引\nSELECT * FROM table USE INDEX (index_name) WHERE condition; STRAIGHT_JOIN\n强制按书写顺序连接JOIN表,固定连接顺序\n索引分类 # 按数据结构分类索引 # B+tree索引 Hash索引 Full-text索引 按物理存储分类索引 # 聚簇索引（主键索引） 二级索引（辅助索引） 按字段特性分类索引 # 主键索引 唯一索引 普通索引 前缀索引 按字段个数分类索引 # 单列索引 联合索引 优化 # 覆盖索引 # 这种在二级索引的 B+Tree 就能查询到结果的过程就叫作「覆盖索引」，也就是只需要查一个 B+Tree 就能找到数据\nCREATE INDEX user_name_age ON user(name, age); SELECT name, age FROM user WHERE name = \u0026#39;Tom\u0026#39;; 索引下推 # 索引下推优化（index condition pushdown)， 可以在联合索引遍历过程中，对联合索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数\n# 若存在index(username,age) , 有索引下推就会减少回表,判断username的同时不急着回表再判断age select * from user2 where username like \u0026#39;j%\u0026#39; and age=99; 最左匹配原则 # 使用联合索引时，存在最左匹配原则，也就是按照最左优先的方式进行索引的匹配\n前缀索引 # 是使用某个字段中字符串的前几个字符建立索引\n索引区分度 # 建立联合索引时，要把区分度大的字段排在前面，这样区分度大的字段越有可能被更多的 SQL 使用到。\n区分度=distinct column / count\n区分度大前面,以小驱动大\n主键索引最好是自增的 # 插入一条新记录，都是追加操作，不需要重新移动数据\n索引最好设置为 NOT NULL # 行格式中至少会用 1 字节空间存储 NULL 值列表\n防止索引失效 # 对索引隐式类型转换 "},{"id":36,"href":"/coding/docs/go/gmp/","title":"调度","section":"golang","content":" 优点 # 内存占用 创建和销毀 切换 GMP # GMP含义\nGoRoutine Go协程，是参与调度与执行的最小单位 Machine 系统级线程 Processor 逻辑处理器 关联了的本地可运行G的队列(也称为LRQ)，最多可存放256个G 线程与进程\n线程 独立调度的基本单位 进程 资源拥有的基本单位 变量 # M0 启动程序后的编号为0的主线程 G0 是每次启动一个M都会第一个创建的GoRoutine，G0仅用于负责调度的G 优势 # work stealing # 工作窃取模型。当一个P(逻辑处理器)的任务队列为空时,它可以随机从其他非空P的任务队列偷取任务来执行。这可以最大限度地提高 CPU 的利用率。\nhandle off # 使能M暂时释放对P的控制,让其他等待中的M接管P,从而实现充分利用多核资源。比如一个阻塞的goroutine释放了CPU,M就可以handle off,让其他M的goroutine运行。\n"},{"id":37,"href":"/coding/docs/mysql/lock/","title":"锁","section":"mysql","content":" 死锁 # 操作系统（四）—死锁\n四个必要条件 # 互斥：在一个时间只能有一个进程使用资源。 请求和保持（持有并等待）：进程保持至少一个资源正在等待获取其他进程持有的额外资源。 不可抢占：一个资源只能在进程已经完成了它的任务之后，被自愿释放。 循环等待：存在n个进程，进行循环等待所占资源。 解决 # 死锁预防 死锁避免 死锁检测 死锁恢复 Mysql的锁\nMySQL 有哪些锁？\n根据锁粒度划分\n全局锁 # Flush tables with read lock (FTWRL)\n全局锁主要应用于做全库逻辑备份，这样在备份数据库期间，不会因为数据或表结构的更新，而出现备份文件的数据与预期的不一样\n表级锁 # 表锁 # 表锁的语法\nlock tables … read/write 表锁的例子\n#表级别的共享锁，也就是读锁； lock tables t_student read; #表级别的独占锁，也就是写锁； lock tables t_stuent write; 主动释放\nunlock tables 元数据锁 # meta data lock,MDL\nMDL 锁是系统默认会加的\n对一张表进行 CRUD 操作时，加的是 MDL 读锁； 对一张表做结构变更操作的时候，加的是 MDL 写锁； 例子: 给一个小表加个字段，导致整个库挂了\n事务不提交，就会一直占着 MDL 锁,先暂停 DDL，或者 kill 掉这个长事务 针对热点表,在 alter table 语句里面设定等待时间,如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，拿不到也不要阻塞后面的业务语句，先放弃 # NOWAIT: # 表示如果当前操作得不到表级别的元数据锁,会直接报错,不会等待锁释放。 # 通常情况下ALTER默认采用NOWAIT模式 ALTER TABLE tbl_name NOWAIT add column ... # WAIT: # 表示如果当前操作得不到表级别的元数据锁,会等待锁被释放,而不是直接报错。 # 可以指定等待的最大秒数,如WAIT 30。如果超过设定时间还没获得锁就报错。 ALTER TABLE tbl_name WAIT N add column ... Intention Lock 意向锁 # 锁的英文缩写\nS shared lock // 共享锁 X exclusive lock // 排他锁 IS intention shared lock // 意向共享锁 IX intention exclusive lock // 意向排他锁 # 先在表上加上意向共享锁，然后对读取的记录加共享锁 select ... lock in share mode; # 先表上加上意向独占锁，然后对读取的记录加独占锁 select ... for update; https://dev.mysql.com/doc/refman/8.0/en/innodb-locking.html#innodb-intention-locks\n锁之间的兼容性\nX IX S IS X ❌ ❌ ❌ ❌ IX ❌ ✅ ❌ ✅ S ❌ ❌ ✅ ✅ IS ❌ ✅ ✅ ✅ 意向锁的目的是为了快速判断表里是否有记录被加锁\nAUTO-INC 锁 # AUTO-INC 锁是特殊的表锁机制，锁不是再一个事务提交后才释放，而是再执行完插入语句后就会立即释放。\n行锁 # Record Locks 记录锁 # mysql \u0026gt; begin; mysql \u0026gt; select * from t_test where id = 1 for update; Gap Locks 间隙锁 # # 区域中所有现有值 15 之间的间隙都是锁定的 SELECT c1 FROM t WHERE c1 BETWEEN 10 and 20 FOR UPDATE; 间隙锁之间是兼容的，即两个事务可以同时持有包含共同间隙范围的间隙锁，并不存在互斥关系，因为间隙锁的目的是防止插入幻影记录而提出的。\nNext-Key Locks 临间锁 # 锁定一个范围，并且锁定记录本身\nnext-key lock 是包含间隙锁(Gap Locks)+记录锁(Record Locks)的， 如果一个事务获取了 X 型的 next-key lock， 那么另外一个事务在获取相同范围的 X 型的 next-key lock 时，是会被阻塞的\nInsert Intention Locks 插入意向锁 # 一个事务在插入一条记录的时候，需要判断插入位置是否已被其他事务加了间隙锁（next-key lock 也包含间隙锁）\n如果有的话，插入操作就会发生阻塞，直到拥有间隙锁的那个事务提交为止（释放间隙锁的时刻），在此期间会生成一个插入意向锁，表明有事务想在某个区间插入新记录，但是现在处于等待状态\n锁策略 # 乐观锁 # 增加版本号version字段,在更新时带上version进行判断 悲观锁 # SELECT FOR UPDATE 读多写少使用乐观锁,写多使用悲观锁。\n"},{"id":38,"href":"/coding/docs/ms/ddd/","title":"领域驱动设计","section":"微服务","content":" 极客时间.DDD 实战课\n领域边界 # 在事件风暴中梳理业务过程中的用户操作、事件以及外部依赖关系等，根据这些要素梳理出领域实体等领域对象 根据领域实体之间的业务关联性，将业务紧密相关的实体进行组合形成聚合，同时确定聚合中的聚合根、值对象和实体。在这个图里，聚合之间的边界是第一层边界，它们在同一个微服务实例中运行，这个边界是逻辑边界，所以用虚线表示 根据业务及语义边界等因素，将一个或者多个聚合划定在一个限界上下文内，形成领域模型。在这个图里，限界上下文之间的边界是第二层边界，这一层边界可能就是未来微服务的边界，不同限界上下文内的领域逻辑被隔离在不同的微服务实例中运行，物理上相互隔离，所以是物理边界，边界之间用实线来表示 领域\n这个边界内要解决的业务问题域.\n在研究和解决业务问题时，DDD 会按照一定的规则将业务领域进行细分，当领域细分到一定的程度后，DDD 会将问题范围限定在特定的边界内，在这个边界内建立领域模型，进而用代码实现该领域模型，解决相应的业务问题\n子域\n我们把划分出来的多个子领域称为子域，每个子域对应一个更小的问题域或更小的业务范围\n核心域\n决定产品和公司核心竞争力的子域是核心域，它是业务成功的主要因素和公司的核心竞争力\n通用域\n没有太多个性化的诉求，同时被多个子域使用的通用功能子域是通用域\n支撑域\n子域是必需的，但既不包含决定产品和公司核心竞争力的功能，也不包含通用功能的子域，它就是支撑域.\n限界上下文 Bounded Context\n通用语言定义上下文含义，限界上下文则定义领域边界\n通用语言\n在事件风暴过程中，通过团队交流达成共识的，能够简单、清晰、准确描述业务涵义和规则的语言就是通用语言\n设计过程中我们可以用一些表格，来记录事件风暴和微服务设计过程中产生的领域对象及其属性\nDDD 分析和设计过程中的每一个环节都需要保证限界上下文内术语的统一，在代码模型设计的时侯就要建立领域对象和代码对象的一一映射，从而保证业务模型和代码模型的一致，实现业务语言与代码语言的统一.\n领域边界就是通过限界上下文来定义的.\n我们将限界上下文内的领域模型映射到微服务，就完成了从问题域到软件的解决方案.\n实体 Entity # 在 DDD 中有这样一类对象，它们拥有唯一标识符，且标识符在历经各种状态变更后仍能保持一致。对这些对象而言，重要的不是其属性，而是其延续性和标识，对象的延续性和标识会跨越甚至超出软件的生命周期。我们把这样的对象称为实体\n实体和值对象是组成领域模型的基础单元.\n实体的代码形态\n充血模型 实体的运行形态\n实体以 DO（领域对象）的形式存在，每个实体对象都有唯一的 ID. 值对象 ValueObject # 通过对象属性值来识别的对象，它将多个相关属性组合为一个概念整体.值对象本质上就是一个集.\n在领域建模时，我们可以将部分对象设计为值对象，保留对象的业务涵义，同时又减少了实体的数量；在数据建模时，我们可以将值对象嵌入实体，减少实体表的数量，简化数据库设计.\nDDD 提倡从领域模型设计出发，而不是先设计数据模型.\n聚合 Aggregate # 领域模型内的实体和值对象就好比个体，而能让实体和值对象协同工作的组织就是聚合，它用来确保这些领域对象在实现共同的业务逻辑时，能保证数据的一致性。\n聚合根 AggregateRoot\n如果把聚合比作组织，那聚合根就是这个组织的负责人。聚合根也称为根实体，它不仅是实体，还是聚合的管理者。\n聚合的构建过程 采用事件风暴，根据业务行为，梳理出在投保过程中发生这些行为的所有的实体和值对象，比如投保单、标的、客户、被保人等等。 从众多实体中选出适合作为对象管理者的根实体，也就是聚合根。判断一个实体是否是聚合根，你可以结合以下场景分析：是否有独立的生命周期？是否有全局唯一 ID？是否可以创建或修改其它对象？是否有专门的模块来管这个实体。图中的聚合根分别是投保单和客户实体。 根据业务单一职责和高内聚原则，找出与聚合根关联的所有紧密依赖的实体和值对象。构建出 1 个包含聚合根（唯一）、多个实体和值对象的对象集合，这个集合就是聚合。在图中我们构建了客户和投保这两个聚合。 在聚合内根据聚合根、实体和值对象的依赖关系，画出对象的引用和依赖模型。这里我需要说明一下：投保人和被保人的数据，是通过关联客户 ID 从客户聚合中获取的，在投保聚合里它们是投保单的值对象，这些值对象的数据是客户的冗余数据，即使未来客户聚合的数据发生了变更，也不会影响投保单的值对象数据。从图中我们还可以看出实体之间的引用关系，比如在投保聚合里投保单聚合根引用了报价单实体，报价单实体则引用了报价规则子实体。 多个聚合根据业务语义和上下文一起划分到同一个限界上下文内。 聚合设计原则\n在一致性边界内建模真正的不变条件 设计小聚合 通过唯一标识引用其它聚合 在边界之外使用最终一致性 通过应用层实现跨聚合的服务调用 领域事件 Domain Event # 这种事件发生后通常会导致进一步的业务操作，在 DDD 中这种事件被称为领域事件\n流程\n事件构建和发布 事件数据持久化 事件总线 消息中间件 事件接收和处理 DDD分层架构 # 用户接口层 // 用户、程序、自动化测试和批处理脚本etc. 应用层 // 很薄的一层，理论上不应该有业务规则或逻辑，主要面向用例和流程相关的操作 领域层 // 实现企业核心业务逻辑，通过各种校验手段保证业务的正确性 基础层 // 第三方工具、驱动、消息中间件、网关、文件、缓存以及数据库etc. 依赖倒置 // 实现了各层对基础层的解耦 整洁架构 # 又名\u0026quot;洋葱架构\u0026quot;.整洁架构最主要的原则是依赖原则，它定义了各层的依赖关系，越往里依赖越低，代码级别越高，越是核心能力。外圆代码依赖只能指向内圆，内圆不需要知道外圆的任何情况\n六边形架构 # 应用是通过端口与外部进行交互的\n"}]